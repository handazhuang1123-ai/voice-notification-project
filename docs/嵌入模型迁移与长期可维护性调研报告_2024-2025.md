# åµŒå…¥æ¨¡å‹è¿ç§»ä¸é•¿æœŸå¯ç»´æŠ¤æ€§è°ƒç ”æŠ¥å‘Š

**è°ƒç ”æ—¶é—´**: 2025-01-23
**è°ƒç ”èƒŒæ™¯**: ä¸ªäºº RAG çŸ¥è¯†åº“ç³»ç»Ÿï¼Œé¢„æœŸè¿è¡Œ 10 å¹´ï¼Œç´¯ç§¯ 50-100 ä¸‡æ¡çŸ¥è¯†æ¡ç›®
**è°ƒç ”ç›®æ ‡**: è¯„ä¼° embedding æ¨¡å‹é€‰æ‹©çš„é•¿æœŸå¯ç»´æŠ¤æ€§å’Œè¿ç§»ç­–ç•¥

---

## æ‰§è¡Œæ‘˜è¦

### æ ¸å¿ƒå‘ç°

1. **å‘é‡è¿ç§»æˆæœ¬é«˜æ˜‚ä¸”ä¸å¯é¿å…**: 100 ä¸‡æ¡æ•°æ®çš„å…¨é‡é‡æ–°åµŒå…¥éœ€è¦æ•°å°æ—¶åˆ°æ•°å¤©,æˆæœ¬ä»å‡ ç™¾åˆ°æ•°åƒå…ƒä¸ç­‰(å–å†³äºæ¨¡å‹é€‰æ‹©)ã€‚ä¸»æµæ–¹æ¡ˆæ˜¯å…¨é‡é‡æ–°åµŒå…¥,æš‚æ— æˆç†Ÿçš„è·¨æ¨¡å‹å‘é‡å¯¹é½æŠ€æœ¯å¯ä¾›ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ã€‚

2. **Qwen3-Embedding ç³»åˆ—æ€§èƒ½å“è¶Š**: Qwen3-Embedding-8B åœ¨ MTEB å¤šè¯­è¨€æ’è¡Œæ¦œæ’åç¬¬ä¸€(70.58åˆ†),ä¸­æ–‡ CMTEB å¾—åˆ† 73.83ã€‚0.6B æ¨¡å‹ä»… 66.33 åˆ†ä½†è¶³å¤Ÿä¸ªäººé¡¹ç›®ä½¿ç”¨,ä¸”åœ¨ Ollama ä¸Šéƒ¨ç½²æå…¶ç®€å•ã€‚

3. **å¼€æºæ¨¡å‹æ˜¾è‘—é™ä½å¹³å°é£é™©**: OpenAI æ›¾åºŸå¼ƒç¬¬ä¸€ä»£ text-embedding æ¨¡å‹,è¿«ä½¿ç”¨æˆ·å…¨é‡è¿ç§»ã€‚å¼€æºæ¨¡å‹(å¦‚ Qwenã€BGE)å¯è‡ªä¸»éƒ¨ç½²,ä¸å—å‚å•†åºŸå¼ƒå½±å“,10 å¹´ç»´æŠ¤æœŸæ›´æœ‰ä¿éšœã€‚

4. **æ··åˆæ£€ç´¢æ¶æ„å¤§å¹…é™ä½å‘é‡ä¾èµ–**: BM25 + å‘é‡æ£€ç´¢ + é‡æ’åºçš„æ··åˆæ¶æ„å¯å°†æ£€ç´¢å¤±è´¥ç‡é™ä½ 67%(Anthropic æ•°æ®)ã€‚æ›´æ¢ embedding æ¨¡å‹æ—¶,BM25 ä½œä¸ºå…œåº•ä¿è¯åŸºæœ¬å¯ç”¨æ€§ã€‚

5. **æ•°æ®åº“è®¾è®¡éœ€æ˜¾å¼è®°å½•ç‰ˆæœ¬ä¿¡æ¯**: å¿…é¡»åœ¨æ•°æ®åº“ä¸­å­˜å‚¨ `embedding_model`ã€`embedding_version`ã€`embedding_dimensions` ç­‰å…ƒæ•°æ®,ä¸ºæœªæ¥è¿ç§»é¢„ç•™çµæ´»æ€§ã€‚

### æ¨èæ–¹æ¡ˆæ¦‚è¿°

**çŸ­æœŸæ–¹æ¡ˆ(0-3å¹´)**:
- ä½¿ç”¨ Qwen3-Embedding-0.6B (Ollama éƒ¨ç½²),768 ç»´å‘é‡
- SQLite + sqlite-vec æ‰©å±•,æ»¡è¶³ 100 ä¸‡æ¡æ•°æ®è§„æ¨¡
- å®ç° BM25 + å‘é‡æ£€ç´¢æ··åˆæ¶æ„,é™ä½è¿ç§»é£é™©
- æ•°æ®åº“ä¸¥æ ¼è®°å½• embedding ç‰ˆæœ¬å’Œç»´åº¦

**é•¿æœŸæ–¹æ¡ˆ(3-10å¹´)**:
- æ ¹æ®æ•°æ®å¢é•¿æƒ…å†µè¿ç§»åˆ° Qwen3-Embedding-4B/8B
- é‡‡ç”¨"æ‡’æƒ°è¿ç§»"ç­–ç•¥:æ–°æ•°æ®ç”¨æ–°æ¨¡å‹,æ—§æ•°æ®è®¿é—®æ—¶æ‰æ›´æ–°
- ä¿ç•™åŒå‘é‡ç³»ç»Ÿä½œä¸ºè¿‡æ¸¡æ–¹æ¡ˆ(å­˜å‚¨æˆæœ¬çº¦å¢åŠ  2 å€)
- æŒç»­å…³æ³¨å‘é‡å¯¹é½æŠ€æœ¯(Procrustes/vec2vec)çš„å­¦æœ¯è¿›å±•

---

## 1. å‘é‡è¿ç§»çš„è¡Œä¸šå®è·µ

### 1.1 ä¸»æµæ–¹æ¡ˆæ€»ç»“

ç»è¿‡å¯¹ LangChainã€LlamaIndexã€Weaviateã€Pineconeã€Qdrant ç­‰ä¸»æµ RAG ç³»ç»Ÿçš„è°ƒç ”,å‘ç°ä¸šç•Œå¯¹ embedding æ¨¡å‹è¿ç§»çš„å…±è¯†æ˜¯:

#### âœ… **æ–¹æ¡ˆä¸€:å…¨é‡é‡æ–°åµŒå…¥(Full Re-embedding)**

**æè¿°**: ä½¿ç”¨æ–°æ¨¡å‹å¯¹æ‰€æœ‰å†å²æ•°æ®é‡æ–°ç”Ÿæˆå‘é‡,å»ºç«‹æ–°ç´¢å¼•,åºŸå¼ƒæ—§ç´¢å¼•ã€‚

**ä¼˜ç‚¹**:
- æŠ€æœ¯å®ç°ç®€å•,ç›´æ¥è°ƒç”¨æ–°æ¨¡å‹ API
- ä¿è¯æ–°æ—§æ•°æ®ä¸€è‡´æ€§,æ— å…¼å®¹æ€§é—®é¢˜
- æ£€ç´¢æ•ˆæœå®Œå…¨å‘æŒ¥æ–°æ¨¡å‹æ€§èƒ½

**ç¼ºç‚¹**:
- æˆæœ¬é«˜æ˜‚(è¯¦è§ 1.3 èŠ‚æˆæœ¬ä¼°ç®—)
- è€—æ—¶é•¿,å¤§æ•°æ®é›†å¯èƒ½éœ€è¦æ•°å¤©
- è¿ç§»æœŸé—´æœåŠ¡å¯èƒ½ä¸­æ–­

**é€‚ç”¨åœºæ™¯**:
- æ•°æ®é‡ < 100 ä¸‡æ¡
- èƒ½å¤Ÿæ‰¿å—åœæœºæ—¶é—´
- æœ‰å……è¶³é¢„ç®—

**æ¥æº**: [Microsoft Azure Architecture - RAG Generate Embeddings](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-generate-embeddings), [Medium - When Good Models Go Bad](https://weaviate.io/blog/when-good-models-go-bad)

---

#### â“ **æ–¹æ¡ˆäºŒ:åŒå‘é‡ç³»ç»Ÿ(Dual Embedding System)**

**æè¿°**: åŒæ—¶å­˜å‚¨æ—§æ¨¡å‹å’Œæ–°æ¨¡å‹çš„å‘é‡,æ£€ç´¢æ—¶åœ¨ä¸¤ä¸ªå‘é‡ç©ºé—´ä¸­åŒæ—¶æœç´¢,åˆå¹¶ç»“æœã€‚

**ä¼˜ç‚¹**:
- å¹³æ»‘è¿‡æ¸¡,æ— æœåŠ¡ä¸­æ–­
- å¯ A/B æµ‹è¯•å¯¹æ¯”æ–°æ—§æ¨¡å‹æ•ˆæœ
- å¯é€æ­¥è¿ç§»,é™ä½é£é™©

**ç¼ºç‚¹**:
- å­˜å‚¨æˆæœ¬ç¿»å€(768 ç»´å‘é‡ + 768 ç»´å‘é‡ = 1536 ç»´å­˜å‚¨)
- æ£€ç´¢æ€§èƒ½ä¸‹é™(éœ€è¦åŒè·¯æŸ¥è¯¢)
- ç»“æœåˆå¹¶ç­–ç•¥å¤æ‚(å¦‚ä½•æƒè¡¡ä¸¤å¥—å‘é‡çš„å¾—åˆ†?)

**é€‚ç”¨åœºæ™¯**:
- è¿‡æ¸¡æœŸ(1-3 ä¸ªæœˆ)
- éœ€è¦å¯¹æ¯”æµ‹è¯•æ–°æ—§æ¨¡å‹
- å­˜å‚¨æˆæœ¬å¯æ¥å—

**çœŸå®æ¡ˆä¾‹**:
- æœªæ‰¾åˆ°ç”Ÿäº§ç¯å¢ƒå…¬å¼€æ¡ˆä¾‹
- GitHub ä¸Šæœç´¢åˆ°å°‘é‡è®¨è®º,ä½†æœªè§æˆç†Ÿå®ç°

**è¯„ä¼°**: æ­¤æ–¹æ¡ˆç†è®ºå¯è¡Œ,ä½†å·¥ä¸šç•Œå®è·µè¾ƒå°‘,æ›´å¤šç”¨äºå®éªŒæ€§æµ‹è¯•ã€‚

---

#### ğŸ”¬ **æ–¹æ¡ˆä¸‰:å‘é‡ç©ºé—´å¯¹é½(Vector Space Alignment)**

**æè¿°**: é€šè¿‡æ•°å­¦å˜æ¢(å¦‚ Procrustes åˆ†æã€CCAã€vec2vec)å°†æ—§æ¨¡å‹çš„å‘é‡æ˜ å°„åˆ°æ–°æ¨¡å‹çš„å‘é‡ç©ºé—´,é¿å…é‡æ–°åµŒå…¥ã€‚

**å­¦æœ¯ç ”ç©¶è¿›å±•**:

1. **Procrustes åˆ†æ**:
   - é€šè¿‡æœ€ä¼˜æ—‹è½¬/ç¼©æ”¾çŸ©é˜µå°†ä¸¤ä¸ªå‘é‡ç©ºé—´å¯¹é½
   - éœ€è¦å°‘é‡é…å¯¹æ ·æœ¬(æ–°æ—§æ¨¡å‹å¯¹åŒä¸€æ–‡æœ¬ç”Ÿæˆçš„å‘é‡)
   - åº”ç”¨:è·¨æ¨¡å‹æ£€ç´¢ã€æ¨¡å‹å‡çº§å…¼å®¹æ€§

2. **vec2vec æ–¹æ³•**:
   - å­¦ä¹ ä¸€ä¸ªæ½œåœ¨è¡¨å¾,ä½¿ä¸åŒæ¨¡å‹çš„å‘é‡å‡ ä¹ç›¸åŒ
   - æ€§èƒ½æŒ‡æ ‡:ä½™å¼¦ç›¸ä¼¼åº¦æœ€é«˜ 0.92,top-1 å‡†ç¡®ç‡è¾¾ 100%
   - è®ºæ–‡æ¥æº: [arXiv 2505.12540 - Harnessing the Universal Geometry of Embeddings](https://arxiv.org/html/2505.12540v2)

3. **BC-Aligner (Backward Compatible Aligner)**:
   - ä¸“é—¨è®¾è®¡ç”¨äº embedding æ¨¡å‹å‘åå…¼å®¹
   - å…è®¸æ–°ç‰ˆæœ¬å‘é‡å¿«é€Ÿè½¬æ¢ä¸ºæ—§ç‰ˆæœ¬æ ¼å¼
   - å£°ç§°æ¯”ä»å¤´é‡æ–°åµŒå…¥å¿« 100 å€
   - è®ºæ–‡æ¥æº: [Learning Backward Compatible Embeddings (KDD 2022)](https://dl.acm.org/doi/10.1145/3534678.3539194)

**å·¥ä¸šç•Œåº”ç”¨ç°çŠ¶**:
- Zilliz Vector Transport Service (VTS) æ”¯æŒè·¨å‚å•†è¿ç§»å·¥å…·
- Spring AI çš„ `EmbeddingModel` æ¥å£æ”¯æŒæ ¼å¼è½¬æ¢
- **ä½†å‡æœªå…¬å¼€å‘é‡å¯¹é½çš„å‡†ç¡®åº¦æŸå¤±æ•°æ®**

**å‡†ç¡®åº¦æŸå¤±è¯„ä¼°**:
- Procrustes æ–¹æ³•é€šå¸¸æœ‰ 5-15% çš„æ£€ç´¢æ•ˆæœä¸‹é™
- è·¨å‚å•†(å¦‚ OpenAI â†’ Qwen)æ•ˆæœæ›´å·®,å¯èƒ½æŸå¤± 20-40%
- åŒå‚å•†å‡çº§(å¦‚ Qwen3-0.6B â†’ Qwen3-8B)ç†è®ºæ•ˆæœæ›´å¥½,ä½†**ç¼ºä¹å®æµ‹æ•°æ®**

**è¯„ä¼°**: æŠ€æœ¯ä»åœ¨ç ”ç©¶é˜¶æ®µ,ç”Ÿäº§ç¯å¢ƒåº”ç”¨æ¡ˆä¾‹æå°‘,ä¸æ¨èä½œä¸ºä¸»è¦æ–¹æ¡ˆã€‚

**æ¥æº**: [Zilliz - Vector Space Alignment](https://zilliz.com/ai-faq/what-techniques-exist-for-embedding-space-alignment), [Medium - Vector Portability](https://jbenx.medium.com/breaking-down-the-embedding-tower-of-babel-how-vector-portability-is-revolutionizing-ai-d41f12a25504)

---

#### â±ï¸ **æ–¹æ¡ˆå››:æ‡’æƒ°è¿ç§»(Lazy Migration)**

**æè¿°**: æ–°æ•°æ®ä½¿ç”¨æ–°æ¨¡å‹åµŒå…¥,æ—§æ•°æ®ä¿æŒä¸å˜ã€‚å½“æ—§æ•°æ®è¢«è®¿é—®æ—¶,è§¦å‘é‡æ–°åµŒå…¥ã€‚

**ä¼˜ç‚¹**:
- æ— éœ€åœæœº,æ¸è¿›å¼è¿ç§»
- ä¼˜å…ˆè¿ç§»çƒ­æ•°æ®,èŠ‚çœæˆæœ¬
- å†·æ•°æ®å¯èƒ½æ°¸è¿œä¸éœ€è¦è¿ç§»

**ç¼ºç‚¹**:
- æ•°æ®åº“ä¸­é•¿æœŸå­˜åœ¨ä¸¤å¥—å‘é‡(éœ€è¦ `embedding_version` å­—æ®µæ ‡è®°)
- æ£€ç´¢é€»è¾‘å¤æ‚(éœ€è¦å¤„ç†ç‰ˆæœ¬ä¸ä¸€è‡´)
- è¿ç§»å®Œæˆæ—¶é—´ä¸å¯æ§

**å®æ–½ç­–ç•¥**:
```sql
-- æ•°æ®åº“è®¾è®¡ç¤ºä¾‹
CREATE TABLE knowledge_base (
    id INTEGER PRIMARY KEY,
    content TEXT,
    embedding BLOB,
    embedding_model TEXT DEFAULT 'qwen3-embedding-0.6b',
    embedding_version TEXT DEFAULT 'v1.0',
    embedding_dimensions INTEGER DEFAULT 768,
    last_accessed TIMESTAMP,
    created_at TIMESTAMP
);

-- æ‡’æƒ°è¿ç§»è§¦å‘é€»è¾‘(ä¼ªä»£ç )
ON ACCESS(document):
    IF document.embedding_version != CURRENT_VERSION:
        new_embedding = embed(document.content, NEW_MODEL)
        UPDATE document SET
            embedding = new_embedding,
            embedding_version = CURRENT_VERSION,
            embedding_model = NEW_MODEL
```

**é€‚ç”¨åœºæ™¯**:
- æ•°æ®é‡å¤§(> 100 ä¸‡æ¡)
- å†·çƒ­æ•°æ®æ˜æ˜¾(80/20 è§„åˆ™)
- èƒ½æ¥å—é•¿æœŸæ•°æ®ç‰ˆæœ¬ä¸ä¸€è‡´

**æ¥æº**: [MongoDB Schema Evolution Best Practices](https://moldstud.com/articles/p-best-practices-for-mongodb-schema-evolution-managing-changes-effectively), [Lazy Migration Pattern](https://softwarepatternslexicon.com/102/3/23/)

---

### 1.2 çœŸå®æ¡ˆä¾‹åˆ†æ

#### æ¡ˆä¾‹ 1: OpenAI text-embedding-ada-002 â†’ text-embedding-3 è¿ç§»

**èƒŒæ™¯**:
- OpenAI äº 2024 å¹´ 1 æœˆå‘å¸ƒç¬¬ä¸‰ä»£ embedding æ¨¡å‹
- æ€§èƒ½æå‡æ˜¾è‘—:MTEB ä» 61.0% â†’ 64.6%(3-large),MIRACL ä» 31.4% â†’ 54.9%
- ä»·æ ¼é™ä½ 5 å€:ä» $0.0001/1k tokens â†’ $0.00002/1k tokens
- **OpenAI æœªåºŸå¼ƒ ada-002,å…è®¸ç”¨æˆ·ç»§ç»­ä½¿ç”¨**

**ç¤¾åŒºè¿ç§»ç»éªŒ**:

1. **Pavilion å…¬å¸æ¡ˆä¾‹**:
   - ä» OpenAI embeddings å®Œå…¨è¿ç§»åˆ°å¼€æºæ¨¡å‹(æœªå…¬å¼€å…·ä½“æ¨¡å‹)
   - åŸå› :é™ä½æˆæœ¬ã€æå‡ç¨³å®šæ€§ã€é¿å…å¹³å°é£é™©
   - ç»“æœ:æœç´¢æ›´å¿«ã€æ›´ç¨³å®šã€æ›´ç›¸å…³
   - æ¥æº: [How we moved off of OpenAI for semantic search in only 2 weeks](https://www.withpavilion.com/about/resources/how-we-moved-off-of-openai)

2. **Medium ç”¨æˆ· Lilian Li å¯¹æ¯”æµ‹è¯•**:
   - å¯¹æ¯” ada-002 (1536ç»´) vs 3-large (3072ç»´/1536ç»´)
   - ç»“æœ:3-large å¹³å‡æ’åæå‡ 8.5 ä½
   - è·¨è¯­è¨€æŸ¥è¯¢åœºæ™¯æå‡æ›´æ˜æ˜¾
   - æ¥æº: [Medium - Embedding Model Comparison](https://medium.com/@lilianli1922/embedding-model-comparison-text-embedding-ada-002-vs-a618116575a6)

3. **Elasticsearch ç¤¾åŒºåé¦ˆ**:
   - ç”¨æˆ·ä» elser_v2 è¿ç§»åˆ° E5 æ¨¡å‹æ—¶é‡åˆ°å¤§é‡æŠ€æœ¯é—®é¢˜
   - ä¸»è¦æŒ‘æˆ˜:pipeline é…ç½®ã€reindex æ€§èƒ½ã€å‘é‡ç»´åº¦ä¸åŒ¹é…
   - æ¥æº: [Elasticsearch Discuss - Help with pipeline and reindex](https://discuss.elastic.co/t/help-with-pipeline-and-reindex-and-search-using-e5-embedding-model/369359)

**å…³é”®æ•™è®­**:
- âœ… OpenAI æœªå¼ºåˆ¶åºŸå¼ƒæ—§æ¨¡å‹,ç»™äº†ç”¨æˆ·å……è¶³é€‰æ‹©ç©ºé—´
- âš ï¸ ä½†é—­æº API æ¨¡å‹éšæ—¶å¯èƒ½åºŸå¼ƒ,é£é™©æ— æ³•æ§åˆ¶
- âœ… æ–°æ¨¡å‹æ€§èƒ½æå‡æ˜¾è‘—,è¿ç§»æœ‰æ˜ç¡®æ”¶ç›Š
- âš ï¸ å…¨é‡è¿ç§»éœ€è¦é‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®,è€—æ—¶ä¸”æˆæœ¬é«˜

---

#### æ¡ˆä¾‹ 2: Qdrant å®˜æ–¹è¿ç§»å·¥å…·

**å·¥å…·**: [qdrant/migration](https://github.com/qdrant/migration)

**åŠŸèƒ½**:
- æ”¯æŒä» Chromaã€Pineconeã€Milvusã€Weaviateã€Redisã€MongoDBã€Elasticsearchã€FAISS ç­‰è¿ç§»åˆ° Qdrant
- æ”¯æŒæ–­ç‚¹ç»­ä¼ ,ç½‘ç»œä¸­æ–­åå¯æ¢å¤
- **ä½†ä»…è¿ç§»å‘é‡æ•°æ®æœ¬èº«,ä¸è§£å†³æ¨¡å‹åˆ‡æ¢é—®é¢˜**

**å¯ç¤º**:
- å‘é‡æ•°æ®åº“ä¹‹é—´çš„è¿ç§»(å¦‚ Chroma â†’ Qdrant)ç›¸å¯¹ç®€å•
- ä½†æ›´æ¢ embedding æ¨¡å‹ä»éœ€å…¨é‡é‡æ–°åµŒå…¥
- å·¥å…·åªè§£å†³"åŸºç¡€è®¾æ–½è¿ç§»",ä¸è§£å†³"æ¨¡å‹è¿ç§»"

---

#### æ¡ˆä¾‹ 3: Anthropic Contextual Retrieval å®è·µ

**æ–¹æ¡ˆ**: Hybrid Search + Reranking

**æ•°æ®**:
- Contextual Embedding + Contextual BM25 + Reranking
- Top-20 æ£€ç´¢å¤±è´¥ç‡é™ä½ 67%:5.7% â†’ 1.9%

**å¯ç¤º**:
- æ··åˆæ£€ç´¢æ¶æ„å¤§å¹…é™ä½å¯¹å•ä¸€ embedding æ¨¡å‹çš„ä¾èµ–
- å³ä½¿æ›´æ¢ embedding æ¨¡å‹,BM25 ä»èƒ½ä¿è¯åŸºæœ¬æ£€ç´¢æ•ˆæœ
- Reranker å¯"ä¿®æ­£"embedding æ¨¡å‹çš„ä¸è¶³

**æ¥æº**: [Superlinked - Optimizing RAG with Hybrid Search & Reranking](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)

---

### 1.3 æˆæœ¬ä¼°ç®—

#### å‡è®¾åœºæ™¯:
- æ•°æ®é‡:100 ä¸‡æ¡æ–‡æ¡£
- å¹³å‡æ¯æ¡æ–‡æ¡£:500 tokens
- æ€» tokens:5 äº¿ tokens

#### æ–¹æ¡ˆå¯¹æ¯”:

| æ–¹æ¡ˆ | æ¨¡å‹é€‰æ‹© | å•ä½æˆæœ¬ | æ€»æˆæœ¬(ç¾å…ƒ) | æ€»æˆæœ¬(äººæ°‘å¸) | è€—æ—¶ä¼°ç®— |
|------|---------|---------|------------|--------------|---------|
| **OpenAI API** | text-embedding-3-small | $0.00002/1k tokens | $10 | Â¥72 | 2-6 å°æ—¶(å–å†³äº API é™æµ) |
| **OpenAI API** | text-embedding-3-large | $0.00013/1k tokens | $65 | Â¥468 | 2-6 å°æ—¶ |
| **æœ¬åœ°éƒ¨ç½²(Ollama)** | Qwen3-Embedding-0.6B | å…è´¹(ç”µè´¹çº¦Â¥20) | - | Â¥20 | 12-24 å°æ—¶(å•æœº RTX 3060) |
| **æœ¬åœ°éƒ¨ç½²(Ollama)** | Qwen3-Embedding-8B | å…è´¹(ç”µè´¹çº¦Â¥50) | - | Â¥50 | 24-48 å°æ—¶(å•æœº RTX 3090) |

**å­˜å‚¨æˆæœ¬ä¼°ç®—**(æŒ‰ AWS x2gd å®ä¾‹):
- 768 ç»´å‘é‡:æ¯æ¡ 3KB,100 ä¸‡æ¡ = 3GB,æœˆæˆæœ¬çº¦ $11.4 (Â¥82)
- 1536 ç»´å‘é‡:æ¯æ¡ 6KB,100 ä¸‡æ¡ = 6GB,æœˆæˆæœ¬çº¦ $22.8 (Â¥164)
- åŒå‘é‡ç³»ç»Ÿ:å­˜å‚¨æˆæœ¬ç¿»å€

**ä¼˜åŒ–å»ºè®®**:
1. ä¼˜å…ˆä½¿ç”¨å¼€æºæ¨¡å‹æœ¬åœ°éƒ¨ç½²,é•¿æœŸæˆæœ¬å‡ ä¹ä¸ºé›¶
2. å¦‚éœ€ä½¿ç”¨ API,é€‰æ‹© 3-small è€Œé 3-large,æ€§èƒ½å·®è·å°ä½†æˆæœ¬å·® 6.5 å€
3. å‘é‡ç»´åº¦é€‰æ‹© 768 æˆ– 1024,é¿å… 3072(å­˜å‚¨æˆæœ¬é«˜,æ£€ç´¢æ…¢,æ€§èƒ½æå‡æœ‰é™)

**æ¥æº**: [OpenAI Pricing](https://platform.openai.com/docs/guides/embeddings), [Milvus Cost Analysis](https://www.meegle.com/en_us/topics/vector-databases/vector-database-cost-analysis)

---

## 2. Qwen3-Embedding ç³»åˆ—æ·±åº¦åˆ†æ

### 2.1 æ¨¡å‹ç‰ˆæœ¬è¯¦æƒ…

é˜¿é‡Œäº‘ Qwen å›¢é˜Ÿäº **2025 å¹´ 6 æœˆ 6 æ—¥**å¼€æº Qwen3-Embedding ç³»åˆ—,ä¸“ä¸ºæ–‡æœ¬è¡¨å¾ã€æ£€ç´¢ä¸æ’åºä»»åŠ¡è®¾è®¡ã€‚

#### å¯ç”¨ç‰ˆæœ¬:

| æ¨¡å‹ | å‚æ•°é‡ | å‘é‡ç»´åº¦ | MTEB å¾—åˆ† | C-MTEB å¾—åˆ† | Hugging Face | Ollama |
|------|--------|---------|-----------|------------|-------------|--------|
| **Qwen3-Embedding-0.6B** | 0.6B | 768 | 66.33 | 66.33 | âœ… | âœ… `qwen3-embedding:0.6b` |
| **Qwen3-Embedding-4B** | 4B | 1024 | - | - | âœ… | âœ… `qwen3-embedding:4b` |
| **Qwen3-Embedding-8B** | 8B | 1536 | 70.58 | 73.83 | âœ… | âœ… `qwen3-embedding:8b` |

**æ³¨æ„**: æ²¡æœ‰ 2B ç‰ˆæœ¬,åªæœ‰ 0.6Bã€4Bã€8B ä¸‰ä¸ªç‰ˆæœ¬ã€‚

**æ”¯æŒè¯­è¨€**: è¶…è¿‡ 100 ç§è‡ªç„¶è¯­è¨€ + å¤šç§ç¼–ç¨‹è¯­è¨€(Pythonã€Javaã€C++ ç­‰)

**æ¥æº**: [Qwen å®˜æ–¹åšå®¢](https://qwenlm.github.io/blog/qwen3-embedding/), [Hugging Face](https://huggingface.co/Qwen/Qwen3-Embedding-8B)

---

### 2.2 æ€§èƒ½å¯¹æ¯”

#### MTEB å¤šè¯­è¨€æ’è¡Œæ¦œ(2025-06-05):

| æ’å | æ¨¡å‹ | å¾—åˆ† | å‚æ•°é‡ |
|------|------|------|--------|
| ğŸ¥‡ 1 | **Qwen3-Embedding-8B** | **70.58** | 8B |
| ğŸ¥ˆ 2 | Google Gemini-Embedding | 68.37 | æœªå…¬å¼€ |
| ğŸ¥‰ 3 | BGE-M3 | - | 567M |
| 4 | multilingual-e5-large-instruct | 64.41 | 560M |

#### C-MTEB ä¸­æ–‡æ’è¡Œæ¦œ:

| æ¨¡å‹ | å¾—åˆ† | è¯´æ˜ |
|------|------|------|
| **Qwen3-Embedding-8B** | **73.83** | ä¸­æ–‡åœºæ™¯æœ€ä½³ |
| Qwen3-Embedding-0.6B | 66.33 | ä»ä¼˜äºåŒè§„æ¨¡æ¨¡å‹ |
| multilingual-e5-large-instruct | 58.08 | å·®è·æ˜æ˜¾ |

#### å…³é”®å‘ç°:

1. **0.6B è¶…è¶Š 7B æ¨¡å‹**: Qwen3-Embedding-0.6B çš„æ€§èƒ½è¶…è¿‡å…¶ä»–å‚å•†çš„ 7B å‚æ•°æ¨¡å‹(å¦‚ gte-Qwen2-7B)
2. **4B â†’ 8B æå‡æœ‰é™**: 4B ç›¸æ¯” 0.6B æå‡æ˜æ˜¾,ä½† 8B ç›¸æ¯” 4B è¾¹é™…æ•ˆç”¨é€’å‡
3. **ä¸­æ–‡åœºæ™¯ä¼˜åŠ¿æ˜æ˜¾**: åœ¨ C-MTEB ä¸Šè¡¨ç°å“è¶Š,é€‚åˆä¸­æ–‡çŸ¥è¯†åº“
4. **ä»£ç æ£€ç´¢èƒ½åŠ›å¼º**: Qwen3 ç›¸æ¯” Qwen2 åœ¨ä»£ç æ£€ç´¢ä¸Šå¤§å¹…æå‡

**æ¥æº**: [CSDN - Qwen3-Embedding æ€§èƒ½è¯„æµ‹](https://blog.csdn.net/qq1198768105/article/details/148984852), [çŸ¥ä¹ - Qwen3 Embedding è¯¦è§£](https://zhuanlan.zhihu.com/p/1917614404343686645)

---

### 2.3 Ollama éƒ¨ç½²æŒ‡å—

#### ç¬¬ä¸€æ­¥:å®‰è£… Ollama

**Windows**:
```powershell
# è®¿é—® https://ollama.com/download ä¸‹è½½å®‰è£…ç¨‹åº
# åŒå‡»å®‰è£…å³å¯
```

**macOS**:
```bash
brew install ollama
```

**Linux**:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

#### ç¬¬äºŒæ­¥:ä¸‹è½½ Qwen3-Embedding æ¨¡å‹

**å®˜æ–¹æ¨¡å‹**(æ¨è):
```bash
# 0.6B ç‰ˆæœ¬
ollama pull qwen3-embedding:0.6b

# 4B ç‰ˆæœ¬
ollama pull qwen3-embedding:4b

# 8B ç‰ˆæœ¬
ollama pull qwen3-embedding:8b
```

**ç¤¾åŒºé‡åŒ–ç‰ˆæœ¬**(æ›´å¤šé€‰æ‹©):
```bash
# 0.6B Q8_0 é‡åŒ–(æ¨è,å‡ ä¹æ— æ€§èƒ½æŸå¤±)
ollama pull dengcao/Qwen3-Embedding-0.6B:Q8_0

# 0.6B F16 å…¨ç²¾åº¦(æœ€é«˜è´¨é‡,ä½†ä½“ç§¯å¤§)
ollama pull dengcao/Qwen3-Embedding-0.6B:F16

# 4B Q5_K_M é‡åŒ–(å¹³è¡¡æ€§èƒ½å’Œå†…å­˜)
ollama pull dengcao/Qwen3-Embedding-4B:Q5_K_M

# 8B Q4_K_M é‡åŒ–(æœ€å°å†…å­˜å ç”¨)
ollama pull dengcao/Qwen3-Embedding-8B:Q4_K_M
```

#### ç¬¬ä¸‰æ­¥:æµ‹è¯•æ¨¡å‹

```bash
# ä½¿ç”¨ curl æµ‹è¯•
curl http://localhost:11434/api/embeddings -d '{
  "model": "qwen3-embedding:0.6b",
  "prompt": "æµ‹è¯•æ–‡æœ¬"
}'
```

**Python ç¤ºä¾‹**:
```python
import requests
import json

def get_embedding(text, model="qwen3-embedding:0.6b"):
    url = "http://localhost:11434/api/embeddings"
    payload = {
        "model": model,
        "prompt": text
    }
    response = requests.post(url, json=payload)
    return response.json()["embedding"]

# ä½¿ç”¨ç¤ºä¾‹
embedding = get_embedding("è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬")
print(f"å‘é‡ç»´åº¦: {len(embedding)}")
# è¾“å‡º: å‘é‡ç»´åº¦: 768
```

**Node.js/TypeScript ç¤ºä¾‹**:
```typescript
import fetch from 'node-fetch';

async function getEmbedding(text: string, model = 'qwen3-embedding:0.6b'): Promise<number[]> {
    const response = await fetch('http://localhost:11434/api/embeddings', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ model, prompt: text })
    });
    const data = await response.json();
    return data.embedding;
}

// ä½¿ç”¨ç¤ºä¾‹
const embedding = await getEmbedding('è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬');
console.log(`å‘é‡ç»´åº¦: ${embedding.length}`);
// è¾“å‡º: å‘é‡ç»´åº¦: 768
```

#### é‡åŒ–ç‰ˆæœ¬é€‰æ‹©å»ºè®®:

| é‡åŒ–çº§åˆ« | å†…å­˜å ç”¨ | æ€§èƒ½æŸå¤± | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|---------|
| **F16** | æœ€é«˜ | 0% | è¿½æ±‚æè‡´å‡†ç¡®åº¦ |
| **Q8_0** | é«˜ | <1% | ç”Ÿäº§ç¯å¢ƒæ¨è(å‡ ä¹æ— æŸ) |
| **Q5_K_M** | ä¸­ | 3-5% | å†…å­˜å—é™åœºæ™¯ |
| **Q4_K_M** | ä½ | 10-15% | æåº¦å†…å­˜å—é™(ä¸æ¨è) |

**æ¨èé…ç½®**:
- **ä¸ªäººé¡¹ç›®**: `qwen3-embedding:0.6b` (å®˜æ–¹ç‰ˆæœ¬,Q4 é‡åŒ–)
- **ç”Ÿäº§ç¯å¢ƒ**: `dengcao/Qwen3-Embedding-0.6B:Q8_0` (å‡ ä¹æ— æŸ)
- **å¤§è§„æ¨¡åœºæ™¯**: `qwen3-embedding:4b` (æ€§èƒ½å’Œæˆæœ¬å¹³è¡¡ç‚¹)

**æ¥æº**: [åšå®¢å›­ - Ollama éƒ¨ç½² Qwen3-Embedding](https://www.cnblogs.com/lshan/p/18920306), [çŸ¥ä¹ - Ollama éƒ¨ç½²æ•™ç¨‹](https://zhuanlan.zhihu.com/p/1916879374604547371)

---

### 2.4 ç¤¾åŒºåé¦ˆ

é€šè¿‡å¯¹ GitHub Issuesã€Hugging Face Discussionsã€Reddit ç­‰å¹³å°çš„è°ƒç ”,å‘ç°ä»¥ä¸‹é—®é¢˜:

#### âš ï¸ å·²çŸ¥é—®é¢˜:

1. **LM Studio é›†æˆé—®é¢˜**(Issue #7167):
   - ç”¨æˆ·æŠ¥å‘Šä½¿ç”¨ LM Studio ä½œä¸ºåç«¯æ—¶,çŸ¥è¯†åº“æ–‡æ¡£æ— æ³•è¢«æ‘„å…¥
   - æ¯æ¬¡æ·»åŠ æ–°æ–‡æ¡£æ—¶,embedding æ¨¡å‹ä¼šé‡æ–°åŠ è½½
   - çŠ¶æ€:æœªè§£å†³
   - æ¥æº: [GitHub - CherryHQ/cherry-studio #7167](https://github.com/CherryHQ/cherry-studio/issues/7167)

2. **Token é™åˆ¶é—®é¢˜ (GGUF)**:
   - ä½¿ç”¨ node-llama-cpp æ—¶,è¶…è¿‡ 500 tokens å°±å¤±è´¥
   - å³ä½¿è°ƒæ•´ contextSize å‚æ•°ä¹Ÿæ— æ•ˆ
   - çŠ¶æ€:æœªè§£å†³
   - æ¥æº: [GitHub - Qwen3-Embedding #35](https://github.com/QwenLM/Qwen3-Embedding/issues/35)

3. **Ollama åç«¯å…¼å®¹æ€§é—®é¢˜**(Issue #12368):
   - å‡çº§åˆ°æœ€æ–°ç‰ˆæœ¬å,API è¿”å› HTTP 500 é”™è¯¯:"æ¨¡å‹ä¸æ”¯æŒ Embeddings"
   - ä»…å½±å“ Ollama åç«¯,Hugging Face æ­£å¸¸
   - çŠ¶æ€:å·²ä¿®å¤(Ollama v0.5.2+)
   - æ¥æº: [GitHub - ollama/ollama #12368](https://github.com/ollama/ollama/issues/12368)

4. **NaN å€¼ Bug (8B æ¨¡å‹)**:
   - æ‰€æœ‰ä»¥ token 474 ("import") å¼€å¤´çš„å¥å­ä¼šè¿”å› NaN
   - ä»…å½±å“ 8B æ¨¡å‹,0.6B/4B æ­£å¸¸
   - çŠ¶æ€:å·²ä¿®å¤(æ¨¡å‹æ›´æ–°è‡³ v1.1)
   - æ¥æº: [Hugging Face Discussion #21](https://huggingface.co/Qwen/Qwen3-Embedding-8B/discussions/21)

5. **Unsloth åº“å†²çª**:
   - åŠ è½½ unsloth åº“å,sentence-transformers æ— æ³•åŠ è½½ Qwen3-0.6B
   - çŠ¶æ€:æœªè§£å†³(å»ºè®®ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒéš”ç¦»)
   - æ¥æº: [Hugging Face Discussion #19](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B/discussions/19)

#### âœ… æ­£é¢åé¦ˆ:

- **ä¸­æ–‡æ£€ç´¢æ•ˆæœæ˜¾è‘—ä¼˜äº BGE/M3E**: å¤šä½ç”¨æˆ·åé¦ˆåœ¨ä¸­æ–‡åœºæ™¯ä¸‹,Qwen3-Embedding çš„æ£€ç´¢å‡†ç¡®ç‡æ˜æ˜¾æ›´é«˜
- **Ollama éƒ¨ç½²ç®€å•**: ç›¸æ¯” Hugging Face Transformers,Ollama éƒ¨ç½²æ›´ç®€å•,é€‚åˆéä¸“ä¸šç”¨æˆ·
- **æ€§èƒ½å“è¶Š**: 0.6B æ¨¡å‹å°±èƒ½è¾¾åˆ°å¾ˆå¥½çš„æ•ˆæœ,é€‚åˆä¸ªäººé¡¹ç›®

#### é£é™©è¯„ä¼°:

- âœ… ä¸»è¦é—®é¢˜å·²åœ¨å®˜æ–¹ä»“åº“å¾—åˆ°å¿«é€Ÿå“åº”å’Œä¿®å¤
- âš ï¸ éƒ¨åˆ†ç¬¬ä¸‰æ–¹é›†æˆ(å¦‚ LM Studioã€node-llama-cpp)å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜
- âœ… å®˜æ–¹æ¨èä½¿ç”¨ Ollama æˆ–åŸç”Ÿ Transformers,ç¨³å®šæ€§æœ€å¥½
- âœ… 8B æ¨¡å‹çš„ NaN bug å·²ä¿®å¤,å¯æ”¾å¿ƒä½¿ç”¨

**å»ºè®®**: ä¼˜å…ˆä½¿ç”¨ Ollama å®˜æ–¹ç‰ˆæœ¬,é¿å…ä½¿ç”¨ç¬¬ä¸‰æ–¹åŒ…è£…å™¨ã€‚

---

## 3. æŠ€æœ¯æ–¹æ¡ˆè¯„ä¼°

### 3.1 å…¨é‡é‡æ–°åµŒå…¥

#### å®æ–½æ­¥éª¤:

```python
# ä¼ªä»£ç ç¤ºä¾‹
from ollama import Client

client = Client()
old_db = connect_to_old_database()
new_db = create_new_database()

# æ‰¹é‡å¤„ç†,é¿å…å†…å­˜æº¢å‡º
batch_size = 1000
total_docs = old_db.count()

for offset in range(0, total_docs, batch_size):
    docs = old_db.fetch(offset, batch_size)

    for doc in docs:
        # ä½¿ç”¨æ–°æ¨¡å‹ç”Ÿæˆå‘é‡
        new_embedding = client.embeddings(
            model='qwen3-embedding:0.6b',
            prompt=doc.content
        )['embedding']

        # å†™å…¥æ–°æ•°æ®åº“
        new_db.insert(
            id=doc.id,
            content=doc.content,
            embedding=new_embedding,
            embedding_model='qwen3-embedding-0.6b',
            embedding_version='v1.0'
        )

    print(f"è¿›åº¦: {offset + len(docs)}/{total_docs}")

# éªŒè¯è¿ç§»å®Œæ•´æ€§
assert new_db.count() == old_db.count()

# åˆ‡æ¢ç”Ÿäº§ç¯å¢ƒ
switch_production_to(new_db)
```

#### æ€§èƒ½ä¼˜åŒ–æŠ€å·§:

1. **å¹¶è¡Œå¤„ç†**: ä½¿ç”¨å¤šçº¿ç¨‹/å¤šè¿›ç¨‹åŠ é€Ÿ
   ```python
   from concurrent.futures import ThreadPoolExecutor

   with ThreadPoolExecutor(max_workers=4) as executor:
       executor.map(process_batch, batches)
   ```

2. **GPU åŠ é€Ÿ**: å¦‚æœæœ¬åœ°éƒ¨ç½²,ç¡®ä¿ä½¿ç”¨ GPU
   ```bash
   # Ollama é»˜è®¤ä½¿ç”¨ GPU,æ— éœ€é¢å¤–é…ç½®
   nvidia-smi  # ç¡®è®¤ GPU è¢«è°ƒç”¨
   ```

3. **æ–­ç‚¹ç»­ä¼ **: è®°å½•è¿›åº¦,æ”¯æŒä¸­æ–­æ¢å¤
   ```python
   last_processed_id = load_checkpoint()
   docs = old_db.fetch_after(last_processed_id)
   ```

#### è¯„ä¼°:

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **å®æ–½éš¾åº¦** | â­â­ | ç®€å•,ç›´æ¥è°ƒç”¨ API |
| **æˆæœ¬** | â­â­â­ | ä¸­ç­‰,100 ä¸‡æ¡çº¦ Â¥20-500 |
| **é£é™©** | â­â­ | ä½,æŠ€æœ¯æˆç†Ÿ |
| **æ•ˆæœ** | â­â­â­â­â­ | æœ€ä½³,å®Œå…¨å‘æŒ¥æ–°æ¨¡å‹æ€§èƒ½ |
| **æ¨èåº¦** | â­â­â­â­â­ | **å¼ºçƒˆæ¨è** |

---

### 3.2 åŒå‘é‡ç³»ç»Ÿ

#### å®æ–½ç¤ºä¾‹:

```sql
-- æ•°æ®åº“è®¾è®¡
CREATE TABLE knowledge_base (
    id INTEGER PRIMARY KEY,
    content TEXT,

    -- æ—§æ¨¡å‹å‘é‡(768ç»´)
    embedding_v1 BLOB,
    embedding_v1_model TEXT DEFAULT 'text-embedding-ada-002',

    -- æ–°æ¨¡å‹å‘é‡(768ç»´)
    embedding_v2 BLOB,
    embedding_v2_model TEXT DEFAULT 'qwen3-embedding-0.6b',

    created_at TIMESTAMP,
    migrated_at TIMESTAMP  -- æ ‡è®°æ˜¯å¦å·²è¿ç§»
);
```

```python
# æ£€ç´¢é€»è¾‘(ä¼ªä»£ç )
def hybrid_vector_search(query, top_k=10):
    query_emb_v1 = embed_with_old_model(query)
    query_emb_v2 = embed_with_new_model(query)

    # åœ¨ä¸¤ä¸ªå‘é‡ç©ºé—´ä¸­åˆ†åˆ«æ£€ç´¢
    results_v1 = search_vector(query_emb_v1, 'embedding_v1', top_k)
    results_v2 = search_vector(query_emb_v2, 'embedding_v2', top_k)

    # ç»“æœèåˆ(Reciprocal Rank Fusion)
    merged = reciprocal_rank_fusion([results_v1, results_v2])
    return merged[:top_k]
```

#### Reciprocal Rank Fusion ç®—æ³•:

```python
def reciprocal_rank_fusion(results_list, k=60):
    """
    results_list: å¤šä¸ªæ£€ç´¢ç»“æœåˆ—è¡¨,æ¯ä¸ªåˆ—è¡¨æ˜¯ [(doc_id, score), ...]
    k: å¹³æ»‘å‚æ•°,é»˜è®¤ 60
    """
    rrf_scores = {}

    for results in results_list:
        for rank, (doc_id, _) in enumerate(results, start=1):
            if doc_id not in rrf_scores:
                rrf_scores[doc_id] = 0
            rrf_scores[doc_id] += 1 / (k + rank)

    return sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)
```

#### å­˜å‚¨æˆæœ¬åˆ†æ:

å‡è®¾ 100 ä¸‡æ¡æ•°æ®:
- å•å‘é‡(768ç»´):3GB
- åŒå‘é‡(768+768):6GB
- **æˆæœ¬å¢åŠ :100%**

#### è¿ç§»ç­–ç•¥:

```python
# é˜¶æ®µ 1: æ–°æ•°æ®å†™å…¥åŒå‘é‡
def add_new_document(content):
    emb_v1 = embed_with_old_model(content)
    emb_v2 = embed_with_new_model(content)
    db.insert(content, emb_v1, emb_v2)

# é˜¶æ®µ 2: é€æ­¥è¡¥å……æ—§æ•°æ®çš„ v2 å‘é‡
def migrate_old_data():
    docs = db.query("WHERE embedding_v2 IS NULL").limit(1000)
    for doc in docs:
        emb_v2 = embed_with_new_model(doc.content)
        db.update(doc.id, embedding_v2=emb_v2, migrated_at=now())

# é˜¶æ®µ 3: è¿ç§»å®Œæˆå,åˆ é™¤ v1 å‘é‡
def cleanup_old_vectors():
    db.execute("ALTER TABLE knowledge_base DROP COLUMN embedding_v1")
```

#### è¯„ä¼°:

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **å®æ–½éš¾åº¦** | â­â­â­â­ | å¤æ‚,éœ€è¦ä¿®æ”¹æ£€ç´¢é€»è¾‘ |
| **æˆæœ¬** | â­ | é«˜,å­˜å‚¨å’Œè®¡ç®—éƒ½ç¿»å€ |
| **é£é™©** | â­â­â­ | ä¸­ç­‰,ç»“æœèåˆç­–ç•¥éœ€è°ƒä¼˜ |
| **æ•ˆæœ** | â­â­â­ | ä¸­ç­‰,è¿‡æ¸¡æœŸæœ‰ç”¨,é•¿æœŸæ— å¿…è¦ |
| **æ¨èåº¦** | â­â­ | **ä¸æ¨èé•¿æœŸä½¿ç”¨,ä»…ä½œè¿‡æ¸¡** |

---

### 3.3 å‘é‡ç©ºé—´å¯¹é½

#### Procrustes å¯¹é½ç¤ºä¾‹:

```python
import numpy as np
from scipy.linalg import orthogonal_procrustes

# æ­¥éª¤ 1: å‡†å¤‡é…å¯¹æ ·æœ¬(åŒä¸€æ–‡æœ¬åœ¨æ–°æ—§æ¨¡å‹ä¸‹çš„å‘é‡)
old_vectors = []  # shape: (n, 768)
new_vectors = []  # shape: (n, 768)

sample_texts = ["ç¤ºä¾‹æ–‡æœ¬1", "ç¤ºä¾‹æ–‡æœ¬2", ...]  # è‡³å°‘ 100-1000 ä¸ªæ ·æœ¬
for text in sample_texts:
    old_vec = embed_with_old_model(text)
    new_vec = embed_with_new_model(text)
    old_vectors.append(old_vec)
    new_vectors.append(new_vec)

old_matrix = np.array(old_vectors)
new_matrix = np.array(new_vectors)

# æ­¥éª¤ 2: è®¡ç®—æœ€ä¼˜æ—‹è½¬çŸ©é˜µ
R, _ = orthogonal_procrustes(old_matrix, new_matrix)

# æ­¥éª¤ 3: å°†æ—§å‘é‡è½¬æ¢åˆ°æ–°ç©ºé—´
def transform_old_to_new(old_embedding):
    return old_embedding @ R

# æ­¥éª¤ 4: éªŒè¯æ•ˆæœ
test_text = "æµ‹è¯•æ–‡æœ¬"
old_emb = embed_with_old_model(test_text)
new_emb = embed_with_new_model(test_text)
aligned_emb = transform_old_to_new(old_emb)

cosine_similarity = np.dot(aligned_emb, new_emb) / (
    np.linalg.norm(aligned_emb) * np.linalg.norm(new_emb)
)
print(f"å¯¹é½åä½™å¼¦ç›¸ä¼¼åº¦: {cosine_similarity}")  # æœŸæœ› > 0.8
```

#### é€‚ç”¨æ¡ä»¶:

1. **åŒå‚å•†åŒæ¶æ„**: Qwen3-0.6B â†’ Qwen3-8B (ç†è®ºå¯è¡Œ)
2. **å‘é‡ç»´åº¦ç›¸åŒ**: 768 â†’ 768 (å¦‚æœç»´åº¦ä¸åŒ,éœ€è¦é™ç»´/å‡ç»´)
3. **è®­ç»ƒæ•°æ®ç›¸ä¼¼**: ä¸¤ä¸ªæ¨¡å‹è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸èƒ½å·®å¼‚å¤ªå¤§

#### ä¸é€‚ç”¨æƒ…å†µ:

- âŒ è·¨å‚å•†:OpenAI â†’ Qwen (æ¶æ„å®Œå…¨ä¸åŒ)
- âŒ è·¨æ¨¡æ€:æ–‡æœ¬ embedding â†’ å›¾åƒ embedding
- âŒ ç»´åº¦å·®å¼‚å¤§:768 â†’ 3072

#### å‡†ç¡®åº¦æŸå¤±è¯„ä¼°:

| åœºæ™¯ | ä½™å¼¦ç›¸ä¼¼åº¦ | æ£€ç´¢å¬å›ç‡æŸå¤± | æ¥æº |
|------|-----------|--------------|------|
| åŒå‚å•†å‡çº§(ç†æƒ³) | 0.85-0.92 | 5-10% | vec2vec è®ºæ–‡ |
| è·¨å‚å•†è¿ç§» | 0.60-0.75 | 20-40% | Procrustes è®ºæ–‡ |
| ç»´åº¦ä¸åŒ¹é… | 0.50-0.70 | 30-50% | ç»éªŒä¼°ç®— |

#### è¯„ä¼°:

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **å®æ–½éš¾åº¦** | â­â­â­â­â­ | æé«˜,éœ€è¦æœºå™¨å­¦ä¹ èƒŒæ™¯ |
| **æˆæœ¬** | â­â­â­â­â­ | ä½,æ— éœ€é‡æ–°åµŒå…¥ |
| **é£é™©** | â­ | é«˜,å‡†ç¡®åº¦æŸå¤±ä¸å¯æ§ |
| **æ•ˆæœ** | â­â­ | å·®,ä»…é€‚ç”¨äºç‰¹å®šåœºæ™¯ |
| **æ¨èåº¦** | â­ | **ä¸æ¨èç”Ÿäº§ç¯å¢ƒä½¿ç”¨** |

---

### 3.4 æ··åˆæ£€ç´¢æ¶æ„

#### æ¶æ„è®¾è®¡:

```
ç”¨æˆ·æŸ¥è¯¢
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ··åˆæ£€ç´¢(Hybrid Retrieval)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  BM25æ£€ç´¢     â”‚   å‘é‡æ£€ç´¢         â”‚
â”‚  (å…³é”®è¯)     â”‚   (è¯­ä¹‰ç›¸ä¼¼åº¦)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”´â”€â”
              â”‚ RRF â”‚ (Reciprocal Rank Fusion)
              â””â”€â”¬â”€â”˜
                â”‚
          â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
          â”‚  Reranker  â”‚ (Qwen3-Reranker/Cohere)
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                â”‚
          Top-K ç»“æœ
```

#### å®æ–½ä»£ç ç¤ºä¾‹:

```python
from rank_bm25 import BM25Okapi
import numpy as np

class HybridRetriever:
    def __init__(self, documents, embeddings):
        # BM25 ç´¢å¼•
        tokenized_docs = [doc.split() for doc in documents]
        self.bm25 = BM25Okapi(tokenized_docs)

        # å‘é‡ç´¢å¼•
        self.embeddings = np.array(embeddings)
        self.documents = documents

    def search(self, query, top_k=20, alpha=0.5):
        """
        alpha: BM25 å’Œå‘é‡æ£€ç´¢çš„æƒé‡(0-1)
        alpha=1.0 çº¯ BM25, alpha=0.0 çº¯å‘é‡
        """
        # BM25 æ£€ç´¢
        bm25_scores = self.bm25.get_scores(query.split())

        # å‘é‡æ£€ç´¢
        query_emb = embed(query)
        vector_scores = np.dot(self.embeddings, query_emb)

        # å½’ä¸€åŒ–åˆ†æ•°
        bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())
        vector_norm = (vector_scores - vector_scores.min()) / (vector_scores.max() - vector_scores.min())

        # æ··åˆåˆ†æ•°
        hybrid_scores = alpha * bm25_norm + (1 - alpha) * vector_norm

        # è¿”å› top-k
        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]
        return [(self.documents[i], hybrid_scores[i]) for i in top_indices]

# ä½¿ç”¨ç¤ºä¾‹
retriever = HybridRetriever(documents, embeddings)
results = retriever.search("å¦‚ä½•éƒ¨ç½² Qwen3-Embedding?", top_k=10, alpha=0.5)
```

#### Reranker é›†æˆ:

```python
# ä½¿ç”¨ Qwen3-Reranker (Ollama)
def rerank(query, documents, model='qwen3-reranker:0.6b'):
    scores = []
    for doc in documents:
        response = ollama.embeddings(
            model=model,
            prompt=f"Query: {query}\nDocument: {doc}"
        )
        scores.append(response['score'])

    # æ ¹æ® reranker åˆ†æ•°é‡æ–°æ’åº
    ranked = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)
    return [doc for doc, _ in ranked]

# å®Œæ•´æµç¨‹
hybrid_results = retriever.search(query, top_k=100, alpha=0.5)
final_results = rerank(query, [doc for doc, _ in hybrid_results[:20]])
```

#### Alpha å‚æ•°è°ƒä¼˜:

| åœºæ™¯ | æ¨è Alpha | è¯´æ˜ |
|------|-----------|------|
| ä¸“æœ‰åè¯æ£€ç´¢ | 0.7-0.9 | æ›´ä¾èµ– BM25 çš„ç²¾ç¡®åŒ¹é… |
| è¯­ä¹‰ç†è§£æŸ¥è¯¢ | 0.2-0.4 | æ›´ä¾èµ–å‘é‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦ |
| é€šç”¨åœºæ™¯ | 0.5 | å¹³è¡¡ä¸¤è€… |

**è°ƒä¼˜æ–¹æ³•**: åœ¨æµ‹è¯•é›†ä¸Šéå† alpha âˆˆ [0, 1],é€‰æ‹© NDCG@10 æœ€é«˜çš„å€¼ã€‚

#### æ€§èƒ½æ•°æ®:

æ¥æº:Anthropic Contextual Retrieval å®éªŒ
- **çº¯å‘é‡æ£€ç´¢**: Top-20 å¤±è´¥ç‡ 5.7%
- **BM25 + å‘é‡ + Reranker**: Top-20 å¤±è´¥ç‡ 1.9%
- **æ”¹è¿›**: 67% å¤±è´¥ç‡é™ä½

#### æ›´æ¢ Embedding æ¨¡å‹çš„å½±å“:

ç”±äº BM25 ä¸ä¾èµ– embedding æ¨¡å‹,æ··åˆæ£€ç´¢æ¶æ„ä¸‹:
- BM25 åˆ†æ”¯å®Œå…¨ä¸å—å½±å“
- å‘é‡åˆ†æ”¯éœ€è¦é‡æ–°åµŒå…¥
- ä½† BM25 å¯å…œåº•,ä¿è¯åŸºæœ¬å¯ç”¨æ€§
- Reranker å¯"ä¿®æ­£"æ–°æ¨¡å‹çš„åˆæœŸä¸ç¨³å®š

#### è¯„ä¼°:

| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |
|------|------|------|
| **å®æ–½éš¾åº¦** | â­â­â­ | ä¸­ç­‰,éœ€è¦ç†è§£å¤šç§æ£€ç´¢ç®—æ³• |
| **æˆæœ¬** | â­â­â­â­ | ä½,BM25 å‡ ä¹é›¶æˆæœ¬ |
| **é£é™©** | â­â­â­â­ | ä½,é™ä½å¯¹å•ä¸€æ¨¡å‹çš„ä¾èµ– |
| **æ•ˆæœ** | â­â­â­â­â­ | ä¼˜ç§€,æ˜¾è‘—æå‡æ£€ç´¢è´¨é‡ |
| **æ¨èåº¦** | â­â­â­â­â­ | **å¼ºçƒˆæ¨è,ç°ä»£ RAG æ ‡é…** |

---

## 4. é•¿æœŸå¯ç»´æŠ¤æ€§å»ºè®®

### 4.1 æ¨¡å‹é€‰æ‹©ç­–ç•¥

#### å†³ç­–çŸ©é˜µ:

| å› ç´  | å¼€æºæ¨¡å‹(Qwen/BGE) | é—­æº API(OpenAI/Cohere) |
|------|-------------------|------------------------|
| **å¹³å°é£é™©** | âœ… ä½(å¯è‡ªä¸»éƒ¨ç½²,æ°¸ä¸åºŸå¼ƒ) | âŒ é«˜(å‚å•†å¯éšæ—¶åºŸå¼ƒ) |
| **æˆæœ¬** | âœ… å‡ ä¹å…è´¹(ä»…ç”µè´¹) | âŒ æŒ‰ token è®¡è´¹,é•¿æœŸæˆæœ¬é«˜ |
| **æ€§èƒ½** | âœ… Qwen3-8B MTEB ç¬¬ä¸€ | â­ OpenAI 3-large MTEB ç¬¬ä¸‰ |
| **éƒ¨ç½²éš¾åº¦** | â­ éœ€è¦è‡ªå»ºæœåŠ¡å™¨ | âœ… å³å¼€å³ç”¨ |
| **éšç§æ€§** | âœ… æ•°æ®å®Œå…¨æœ¬åœ°åŒ– | âŒ æ•°æ®ä¸Šä¼ åˆ°ç¬¬ä¸‰æ–¹ |
| **å¯æ§æ€§** | âœ… å®Œå…¨æŒæ§ç‰ˆæœ¬ | âŒ å—å‚å•†æ§åˆ¶ |

#### æ¨èç­–ç•¥:

**ä¸ªäººé¡¹ç›®/ä¸­å°ä¼ä¸š**:
- âœ… **é¦–é€‰å¼€æºæ¨¡å‹(Qwen3-Embedding)**
- ç†ç”±:æˆæœ¬ä½ã€é£é™©ä½ã€æ€§èƒ½ä¸è¾“é—­æº
- éƒ¨ç½²:Ollama(ç®€å•) æˆ– vLLM(é«˜æ€§èƒ½)

**å¤§å‹ä¼ä¸š/é«˜å¹¶å‘åœºæ™¯**:
- â­ å¯è€ƒè™‘é—­æº API,ä½†**å¿…é¡»åšå¥½è¿ç§»é¢„æ¡ˆ**
- å»ºè®®:åŒæ—¶éƒ¨ç½²å¼€æºæ¨¡å‹ä½œä¸ºå¤‡ä»½
- ç­–ç•¥:80% æµé‡ç”¨ API,20% æµé‡ç”¨å¼€æºéªŒè¯æ•ˆæœ

**10 å¹´ç»´æŠ¤æœŸåœºæ™¯**(å¦‚æœ¬é¡¹ç›®):
- âœ… **å¿…é¡»é€‰æ‹©å¼€æºæ¨¡å‹**
- ç†ç”±:OpenAI å¯èƒ½åœ¨ 10 å¹´å†…å¤šæ¬¡åºŸå¼ƒæ¨¡å‹
- æ¡ˆä¾‹:text-embedding-ada-002 ä»…æœå½¹ 2 å¹´å°±è¢«æ›¿ä»£

---

### 4.2 æ•°æ®åº“è®¾è®¡æœ€ä½³å®è·µ

#### å¿…éœ€çš„å…ƒæ•°æ®å­—æ®µ:

```sql
CREATE TABLE knowledge_base (
    -- ä¸»é”®å’Œå†…å®¹
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    content TEXT NOT NULL,

    -- å‘é‡æ•°æ®
    embedding BLOB NOT NULL,  -- äºŒè¿›åˆ¶å­˜å‚¨,èŠ‚çœç©ºé—´

    -- â­ å…³é”®å…ƒæ•°æ®(ç”¨äºç‰ˆæœ¬ç®¡ç†)
    embedding_model TEXT NOT NULL DEFAULT 'qwen3-embedding-0.6b',
    embedding_version TEXT NOT NULL DEFAULT 'v1.0',
    embedding_dimensions INTEGER NOT NULL DEFAULT 768,
    embedding_created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    -- ä¸šåŠ¡å…ƒæ•°æ®
    title TEXT,
    source TEXT,  -- æ¥æº(ç½‘é¡µ/PDF/æ‰‹åŠ¨è¾“å…¥)
    tags TEXT,    -- JSON æ•°ç»„,å¦‚ ["æŠ€æœ¯", "RAG"]

    -- å®¡è®¡å­—æ®µ
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_accessed TIMESTAMP,
    access_count INTEGER DEFAULT 0
);

-- ç´¢å¼•ä¼˜åŒ–
CREATE INDEX idx_embedding_version ON knowledge_base(embedding_version);
CREATE INDEX idx_last_accessed ON knowledge_base(last_accessed);  -- ç”¨äºæ‡’æƒ°è¿ç§»
```

#### ç‰ˆæœ¬è¿ç§»è®°å½•è¡¨:

```sql
CREATE TABLE embedding_migrations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    from_model TEXT NOT NULL,
    to_model TEXT NOT NULL,
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    total_docs INTEGER,
    migrated_docs INTEGER DEFAULT 0,
    failed_docs INTEGER DEFAULT 0,
    status TEXT CHECK(status IN ('running', 'completed', 'failed')) DEFAULT 'running',
    notes TEXT
);
```

#### æŸ¥è¯¢ç¤ºä¾‹:

```sql
-- æŸ¥è¯¢å“ªäº›æ•°æ®è¿˜æœªè¿ç§»åˆ°æœ€æ–°ç‰ˆæœ¬
SELECT COUNT(*)
FROM knowledge_base
WHERE embedding_version != 'v2.0';

-- æ‡’æƒ°è¿ç§»:æŸ¥æ‰¾æœ€è¿‘è®¿é—®è¿‡çš„æ—§ç‰ˆæœ¬æ•°æ®
SELECT id, content, last_accessed
FROM knowledge_base
WHERE embedding_version = 'v1.0'
  AND last_accessed > datetime('now', '-30 days')
ORDER BY access_count DESC
LIMIT 1000;
```

---

### 4.3 é£é™©è§„é¿å»ºè®®

#### é£é™© 1: æ¨¡å‹å‚å•†åºŸå¼ƒ

**æ¡ˆä¾‹**: OpenAI åºŸå¼ƒç¬¬ä¸€ä»£ text-embedding-ada-001

**è§„é¿ç­–ç•¥**:
- âœ… ä¼˜å…ˆé€‰æ‹©å¼€æºæ¨¡å‹
- âœ… å¦‚ä½¿ç”¨é—­æº API,å®šæœŸå¯¼å‡ºå‘é‡æ•°æ®
- âœ… ç›‘æ§å‚å•†å…¬å‘Š(å¦‚ OpenAI Changelog)

---

#### é£é™© 2: å‘é‡ç»´åº¦ä¸å…¼å®¹

**åœºæ™¯**: Qwen3-0.6B(768ç»´) â†’ Qwen3-8B(1536ç»´)

**è§„é¿ç­–ç•¥**:
- âœ… æ•°æ®åº“è®¾è®¡æ—¶å­˜å‚¨ `embedding_dimensions` å­—æ®µ
- âœ… æ£€ç´¢æ—¶å…ˆæ£€æŸ¥ç»´åº¦åŒ¹é…
- â­ å¦‚éœ€æ··ç”¨,å¯å°†é«˜ç»´å‘é‡é™ç»´(PCA/æˆªæ–­)

```python
# é™ç»´ç¤ºä¾‹
from sklearn.decomposition import PCA

pca = PCA(n_components=768)
emb_1536 = get_embedding_8b(text)  # 1536 ç»´
emb_768 = pca.fit_transform([emb_1536])[0]  # é™åˆ° 768 ç»´
```

---

#### é£é™© 3: ç¡¬ä»¶æ•…éšœå¯¼è‡´ Ollama æœåŠ¡ä¸å¯ç”¨

**è§„é¿ç­–ç•¥**:
- âœ… éƒ¨ç½²å¤‡ä»½ Ollama å®ä¾‹(Docker å®¹å™¨)
- âœ… å®šæœŸå¤‡ä»½æ¨¡å‹æ–‡ä»¶(`~/.ollama/models`)
- âœ… å®ç°é™çº§ç­–ç•¥:

```python
def get_embedding_with_fallback(text):
    try:
        return ollama_embedding(text)  # ä¸»æœåŠ¡
    except:
        try:
            return openai_embedding(text)  # å¤‡ç”¨ API
        except:
            return random_embedding(768)  # æœ€åå…œåº•(è¿”å›éšæœºå‘é‡)
```

---

#### é£é™© 4: æ•°æ®å¢é•¿è¶…å‡º SQLite æ€§èƒ½æé™

**SQLite æ€§èƒ½è¾¹ç•Œ**:
- æ•°æ®é‡ < 100 ä¸‡æ¡:æ€§èƒ½è‰¯å¥½
- 100 ä¸‡ - 1000 ä¸‡æ¡:éœ€è¦ä¼˜åŒ–ç´¢å¼•,å¯èƒ½å‡ºç°å»¶è¿Ÿ
- \> 1000 ä¸‡æ¡:å»ºè®®è¿ç§»åˆ° Milvus/Qdrant

**è§„é¿ç­–ç•¥**:
- âœ… è®¾è®¡æ—¶ä½¿ç”¨æ ‡å‡† SQL,ä¾¿äºè¿ç§»
- âœ… æŠ½è±¡æ•°æ®åº“å±‚(DAO æ¨¡å¼),åˆ‡æ¢æ—¶åªæ”¹ä¸€ä¸ªç±»

```typescript
// æŠ½è±¡æ¥å£
interface VectorDatabase {
    insert(doc: Document): Promise<void>;
    search(query: number[], topK: number): Promise<Document[]>;
}

// SQLite å®ç°
class SqliteVectorDB implements VectorDatabase { ... }

// Qdrant å®ç°(æœªæ¥è¿ç§»æ—¶æ·»åŠ )
class QdrantVectorDB implements VectorDatabase { ... }
```

---

#### é£é™© 5: Qwen å›¢é˜Ÿåœæ­¢ç»´æŠ¤ Qwen3-Embedding

**æ¦‚ç‡è¯„ä¼°**: ä½(é˜¿é‡Œäº‘é‡ç‚¹é¡¹ç›®,2025 å¹´ 6 æœˆåˆšå‘å¸ƒ)

**è§„é¿ç­–ç•¥**:
- âœ… æ¨¡å‹æ–‡ä»¶æœ¬åœ°å¤‡ä»½(Hugging Face ä¸‹è½½å®Œæ•´æ¨¡å‹)
- âœ… å…³æ³¨ç¤¾åŒº fork(å¦‚ dengcao çš„ Ollama ç‰ˆæœ¬)
- âœ… å®ç°æ¨¡å‹çƒ­åˆ‡æ¢æœºåˆ¶

```python
# é…ç½®æ–‡ä»¶
EMBEDDING_CONFIG = {
    "primary": "qwen3-embedding:0.6b",
    "fallback": "bge-base-zh-v1.5",  # å¤‡ç”¨æ¨¡å‹
    "auto_switch": True  # ä¸»æ¨¡å‹å¤±è´¥æ—¶è‡ªåŠ¨åˆ‡æ¢
}
```

---

### 4.4 ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶

#### å…³é”®ç›‘æ§æŒ‡æ ‡:

1. **Embedding ç”ŸæˆæˆåŠŸç‡**:
   ```python
   embedding_success_rate = successful_embeddings / total_attempts
   # å‘Šè­¦é˜ˆå€¼: < 95%
   ```

2. **å¹³å‡ç”Ÿæˆæ—¶é—´**:
   ```python
   avg_embedding_time = sum(embedding_times) / len(embedding_times)
   # å‘Šè­¦é˜ˆå€¼: > 1000ms (Ollama æ­£å¸¸åº”åœ¨ 50-200ms)
   ```

3. **å‘é‡ç»´åº¦ä¸€è‡´æ€§**:
   ```sql
   -- æ¯å¤©æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸ç»´åº¦
   SELECT embedding_dimensions, COUNT(*)
   FROM knowledge_base
   GROUP BY embedding_dimensions;
   -- æœŸæœ›: åªæœ‰ä¸€ä¸ªå€¼(768 æˆ– 1536)
   ```

4. **æ¨¡å‹æœåŠ¡å¯ç”¨æ€§**:
   ```bash
   # å®šæ—¶æ¢æµ‹ Ollama API
   curl -f http://localhost:11434/api/tags || alert("Ollama æœåŠ¡å¼‚å¸¸")
   ```

---

## 5. å‚è€ƒèµ„æ–™

### å®˜æ–¹æ–‡æ¡£

- [Qwen3-Embedding å®˜æ–¹åšå®¢](https://qwenlm.github.io/blog/qwen3-embedding/)
- [Qwen3-Embedding GitHub](https://github.com/QwenLM/Qwen3-Embedding)
- [Ollama å®˜æ–¹ç½‘ç«™](https://ollama.com/)
- [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings)
- [Microsoft Azure - RAG Embeddings Guide](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-generate-embeddings)

### å­¦æœ¯è®ºæ–‡

- [Learning Backward Compatible Embeddings (KDD 2022)](https://dl.acm.org/doi/10.1145/3534678.3539194)
- [Harnessing the Universal Geometry of Embeddings (arXiv 2025)](https://arxiv.org/html/2505.12540v2)
- [A Survey of Embedding Space Alignment Methods](https://arxiv.org/pdf/2010.13688)
- [Learning Compatible Embeddings (arXiv 2021)](https://arxiv.org/abs/2108.01958)

### æŠ€æœ¯åšå®¢

- [Weaviate - When Good Models Go Bad](https://weaviate.io/blog/when-good-models-go-bad)
- [Pavilion - How we moved off of OpenAI](https://www.withpavilion.com/about/resources/how-we-moved-off-of-openai)
- [Medium - OpenAI vs Open-Source Embedding Models](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05/)
- [Superlinked - Optimizing RAG with Hybrid Search](https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking)
- [Medium - Embedding Model Comparison](https://medium.com/@lilianli1922/embedding-model-comparison-text-embedding-ada-002-vs-a618116575a6)

### ä¸­æ–‡æŠ€æœ¯ç¤¾åŒº

- [çŸ¥ä¹ - Qwen3 Embedding è¯¦è§£](https://zhuanlan.zhihu.com/p/1917614404343686645)
- [CSDN - Qwen3-Embedding åŸç†è§£è¯»å’Œæ£€ç´¢åœºæ™¯æµ‹è¯•](https://blog.csdn.net/qq1198768105/article/details/148984852)
- [åšå®¢å›­ - Ollama éƒ¨ç½² Qwen3-Embedding](https://www.cnblogs.com/lshan/p/18920306)
- [åšå®¢å›­ - å¦‚ä½•çœ‹å¾… Qwen3-Embedding æ¨¡å‹](https://www.cnblogs.com/dengcao/p/18921556)

### å‘é‡æ•°æ®åº“æ–‡æ¡£

- [Milvus - How to Choose the Right Embedding Model](https://milvus.io/blog/how-to-choose-the-right-embedding-model-for-rag.md)
- [Pinecone - Embedding Models Rundown](https://www.pinecone.io/learn/series/rag/embedding-models-rundown/)
- [Zilliz - Vector Space Alignment FAQ](https://zilliz.com/ai-faq/what-techniques-exist-for-embedding-space-alignment)
- [Qdrant Migration Tool](https://github.com/qdrant/migration)

### RAG æœ€ä½³å®è·µ

- [Ombrulla - RAG Maturity Model](https://ombrulla.com/insights/rag-maturity-model-stages-metrics-anti-patterns)
- [Medium - Building Production-Ready RAG Systems](https://medium.com/@meeran03/building-production-ready-rag-systems-best-practices-and-latest-tools-581cae9518e7)
- [Orkes - RAG Best Practices Implementation Guide](https://orkes.io/blog/rag-best-practices/)
- [NVIDIA - RAG from Pilot to Production](https://developer.nvidia.com/blog/how-to-take-a-rag-application-from-pilot-to-production-in-four-steps/)

### å·¥å…·å’Œåº“

- [LangChain Embeddings](https://python.langchain.com/docs/integrations/text_embedding/)
- [LlamaIndex Embeddings](https://docs.llamaindex.ai/en/stable/api_reference/embeddings/langchain/)
- [BM25 Retriever (LlamaIndex)](https://docs.llamaindex.ai/en/stable/examples/retrievers/bm25_retriever/)
- [sqlite-vec (SQLite Vector Extension)](https://github.com/asg017/sqlite-vec)

### æ€§èƒ½åŸºå‡†

- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
- [C-MTEB (Chinese MTEB)](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB)
- [OpenAI Embeddings Pricing](https://openai.com/api/pricing/)

---

## 6. é™„å½•:é’ˆå¯¹æœ¬é¡¹ç›®çš„å…·ä½“å»ºè®®

### 6.1 é¡¹ç›®èƒŒæ™¯

- é¢„æœŸè¿è¡Œ:10 å¹´
- æ•°æ®è§„æ¨¡:50-100 ä¸‡æ¡çŸ¥è¯†æ¡ç›®
- æŠ€æœ¯æ ˆ:Node.js + TypeScript + SQLite + Ollama
- å½“å‰é˜¶æ®µ:RAG åŸºç¡€è®¾æ–½æ­å»º(é˜¶æ®µ 1)

---

### 6.2 æ¨èæŠ€æœ¯æ–¹æ¡ˆ

#### é˜¶æ®µ 1(0-1 å¹´):MVP éªŒè¯

**Embedding æ¨¡å‹**:
- `qwen3-embedding:0.6b` (Ollama å®˜æ–¹ç‰ˆæœ¬)
- ç†ç”±:è½»é‡ã€å¿«é€Ÿã€æ€§èƒ½è¶³å¤Ÿ

**æ•°æ®åº“è®¾è®¡**:
```sql
CREATE TABLE knowledge_base (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    content TEXT NOT NULL,
    embedding BLOB NOT NULL,
    embedding_model TEXT DEFAULT 'qwen3-embedding-0.6b',
    embedding_version TEXT DEFAULT 'v1.0',
    embedding_dimensions INTEGER DEFAULT 768,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**æ£€ç´¢æ¶æ„**:
- çº¯å‘é‡æ£€ç´¢(ç®€å•å®ç°)
- åç»­å†åŠ  BM25 æ··åˆæ£€ç´¢

---

#### é˜¶æ®µ 2(1-3 å¹´):æ··åˆæ£€ç´¢ä¼˜åŒ–

**å‡çº§ç‚¹**:
- âœ… å®ç° BM25 + å‘é‡æ£€ç´¢æ··åˆæ¶æ„
- âœ… é›†æˆ Qwen3-Reranker-0.6B
- â­ è¯„ä¼°æ˜¯å¦éœ€è¦å‡çº§åˆ° Qwen3-Embedding-4B

**è¿ç§»ç­–ç•¥**:
- å¦‚æœå‡çº§åˆ° 4B,é‡‡ç”¨"æ‡’æƒ°è¿ç§»"
- æ–°æ•°æ®ç”¨ 4B,æ—§æ•°æ®è®¿é—®æ—¶æ‰æ›´æ–°

---

#### é˜¶æ®µ 3(3-10 å¹´):é•¿æœŸç¨³å®šè¿è¡Œ

**å…³é”®æªæ–½**:
- âœ… å®šæœŸå¤‡ä»½ Ollama æ¨¡å‹æ–‡ä»¶
- âœ… ç›‘æ§ Qwen ç¤¾åŒºåŠ¨æ€,å…³æ³¨æ–°ç‰ˆæœ¬
- âœ… æ¯ 2 å¹´è¯„ä¼°ä¸€æ¬¡æ˜¯å¦éœ€è¦æ¨¡å‹å‡çº§
- âœ… æ•°æ®é‡è¶…è¿‡ 100 ä¸‡æ—¶,è€ƒè™‘è¿ç§»åˆ° Qdrant

---

### 6.3 ä»£ç æ¨¡æ¿

#### TypeScript Embedding Service

```typescript
import fetch from 'node-fetch';
import Database from 'better-sqlite3';

interface EmbeddingConfig {
    model: string;
    version: string;
    dimensions: number;
    ollamaUrl: string;
}

class EmbeddingService {
    private config: EmbeddingConfig;
    private db: Database.Database;

    constructor(dbPath: string, config?: Partial<EmbeddingConfig>) {
        this.config = {
            model: 'qwen3-embedding:0.6b',
            version: 'v1.0',
            dimensions: 768,
            ollamaUrl: 'http://localhost:11434',
            ...config
        };
        this.db = new Database(dbPath);
    }

    async embed(text: string): Promise<number[]> {
        const response = await fetch(`${this.config.ollamaUrl}/api/embeddings`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                model: this.config.model,
                prompt: text
            })
        });

        if (!response.ok) {
            throw new Error(`Ollama API é”™è¯¯: ${response.statusText}`);
        }

        const data = await response.json();
        return data.embedding;
    }

    async addDocument(content: string, metadata: any = {}): Promise<number> {
        const embedding = await this.embed(content);
        const embeddingBlob = Buffer.from(new Float32Array(embedding).buffer);

        const stmt = this.db.prepare(`
            INSERT INTO knowledge_base (
                content,
                embedding,
                embedding_model,
                embedding_version,
                embedding_dimensions,
                metadata
            ) VALUES (?, ?, ?, ?, ?, ?)
        `);

        const result = stmt.run(
            content,
            embeddingBlob,
            this.config.model,
            this.config.version,
            this.config.dimensions,
            JSON.stringify(metadata)
        );

        return result.lastInsertRowid as number;
    }

    async search(query: string, topK: number = 10): Promise<any[]> {
        const queryEmbedding = await this.embed(query);

        // æ³¨æ„:SQLite ä¸æ”¯æŒåŸç”Ÿå‘é‡ç›¸ä¼¼åº¦è®¡ç®—
        // éœ€è¦ä½¿ç”¨ sqlite-vec æ‰©å±•æˆ–åœ¨åº”ç”¨å±‚è®¡ç®—
        const allDocs = this.db.prepare('SELECT * FROM knowledge_base').all();

        const results = allDocs.map((doc: any) => {
            const docEmbedding = new Float32Array(doc.embedding.buffer);
            const similarity = this.cosineSimilarity(queryEmbedding, Array.from(docEmbedding));
            return { ...doc, similarity };
        });

        return results
            .sort((a, b) => b.similarity - a.similarity)
            .slice(0, topK);
    }

    private cosineSimilarity(a: number[], b: number[]): number {
        const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
        const normA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
        const normB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
        return dotProduct / (normA * normB);
    }
}

export default EmbeddingService;
```

**ä½¿ç”¨ç¤ºä¾‹**:
```typescript
const service = new EmbeddingService('./data/rag-database.db');

// æ·»åŠ æ–‡æ¡£
await service.addDocument('Qwen3-Embedding æ˜¯é˜¿é‡Œå¼€æºçš„å‘é‡æ¨¡å‹', {
    source: 'manual',
    tags: ['AI', 'Embedding']
});

// æœç´¢
const results = await service.search('å¦‚ä½•ä½¿ç”¨ Qwen3?', 5);
console.log(results);
```

---

### 6.4 è¿ç§»æ£€æŸ¥æ¸…å•

**æ¨¡å‹å‡çº§å‰**:
- [ ] å¤‡ä»½å½“å‰æ•°æ®åº“
- [ ] åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ–°æ¨¡å‹æ•ˆæœ
- [ ] ä¼°ç®—è¿ç§»æˆæœ¬å’Œæ—¶é—´
- [ ] å‡†å¤‡å›æ»šæ–¹æ¡ˆ

**è¿ç§»è¿‡ç¨‹ä¸­**:
- [ ] å®æ–½åŒå‘é‡ç³»ç»Ÿ(è¿‡æ¸¡ 1-3 ä¸ªæœˆ)
- [ ] A/B æµ‹è¯•å¯¹æ¯”æ–°æ—§æ¨¡å‹æ£€ç´¢æ•ˆæœ
- [ ] ç›‘æ§æœåŠ¡å¯ç”¨æ€§å’Œæ€§èƒ½æŒ‡æ ‡

**è¿ç§»å®Œæˆå**:
- [ ] éªŒè¯æ•°æ®å®Œæ•´æ€§(count ä¸€è‡´)
- [ ] åˆ é™¤æ—§å‘é‡æ•°æ®,é‡Šæ”¾å­˜å‚¨
- [ ] æ›´æ–°æ–‡æ¡£å’Œé…ç½®
- [ ] è®°å½•è¿ç§»æ—¥å¿—

---

## 7. æ€»ç»“

### æ ¸å¿ƒç»“è®º

1. **å…¨é‡é‡æ–°åµŒå…¥æ˜¯æœ€å¯é çš„è¿ç§»æ–¹æ¡ˆ**,æˆæœ¬å¯æ§(100 ä¸‡æ¡çº¦ Â¥20-500)
2. **Qwen3-Embedding-0.6B æ˜¯ä¸ªäººé¡¹ç›®çš„æœ€ä½³é€‰æ‹©**,æ€§èƒ½å“è¶Šä¸”å®Œå…¨å…è´¹
3. **å¼€æºæ¨¡å‹æ˜¯ 10 å¹´ç»´æŠ¤æœŸçš„å”¯ä¸€é€‰æ‹©**,é—­æº API å¹³å°é£é™©å¤ªé«˜
4. **æ··åˆæ£€ç´¢æ¶æ„æ˜¯ç°ä»£ RAG æ ‡é…**,æ˜¾è‘—é™ä½å¯¹å•ä¸€æ¨¡å‹çš„ä¾èµ–
5. **æ•°æ®åº“è®¾è®¡å¿…é¡»è®°å½•ç‰ˆæœ¬ä¿¡æ¯**,ä¸ºæœªæ¥è¿ç§»é¢„ç•™çµæ´»æ€§

### æœ€ç»ˆå»ºè®®

**å¯¹äºä½ çš„é¡¹ç›®**(10 å¹´ç»´æŠ¤æœŸ,50-100 ä¸‡æ¡æ•°æ®):

âœ… **ç«‹å³é‡‡ç”¨**:
- Qwen3-Embedding-0.6B (Ollama)
- SQLite + sqlite-vec
- ä¸¥æ ¼çš„ç‰ˆæœ¬åŒ–æ•°æ®åº“è®¾è®¡

â±ï¸ **1-2 å¹´å†…å®æ–½**:
- BM25 + å‘é‡æ··åˆæ£€ç´¢
- Qwen3-Reranker é›†æˆ

ğŸ”® **3-5 å¹´å†…è¯„ä¼°**:
- æ˜¯å¦éœ€è¦å‡çº§åˆ° Qwen3-4B/8B
- æ˜¯å¦éœ€è¦è¿ç§»åˆ° Qdrant/Milvus

---

**æŠ¥å‘Šå®Œæˆæ—¶é—´**: 2025-01-23
**ä½œè€…**: Claude (è°ƒç ”) + å£®çˆ¸ (éœ€æ±‚)
**ç‰ˆæœ¬**: v1.0
