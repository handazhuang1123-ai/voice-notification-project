# åµŒå…¥æ¨¡å‹é€‰å‹è°ƒç ”æŠ¥å‘Šï¼ˆ2024-2025ï¼‰

> é’ˆå¯¹ä¸ªäººå†å²å¯¹è¯RAGç³»ç»Ÿçš„è´¨é‡æœ€ä¼˜åµŒå…¥æ¨¡å‹é€‰æ‹©
>
> è°ƒç ”æ—¥æœŸï¼š2025-01-20
>
> è°ƒç ”äººå‘˜ï¼šClaude (Sonnet 4.5)

---

## æ‰§è¡Œæ‘˜è¦ï¼ˆExecutive Summaryï¼‰

åŸºäº2024-2025å¹´æœ€æ–°ç ”ç©¶ï¼Œ**æ¨èTop 3æ¨¡å‹**ï¼ˆæŒ‰ç»¼åˆè´¨é‡æ’åºï¼‰ï¼š

1. **Qwen3-Embedding-0.6B/4B** - ç»¼åˆæœ€ä¼˜ï¼Œä¸­æ–‡SOTAï¼Œæœ¬åœ°éƒ¨ç½²å‹å¥½
2. **BGE-Large-ZH-V1.5** - ä¸­æ–‡ä¸“ç”¨ä¼˜åŒ–ï¼ŒC-MTEBç¬¬ä¸€
3. **BGE-M3** - å¤šåŠŸèƒ½æ··åˆæ£€ç´¢ï¼Œé•¿æ–‡æœ¬æ”¯æŒ

**é’ˆå¯¹ä½ çš„åœºæ™¯æ¨è**ï¼š
- **åˆæœŸ500æ¡** â†’ **Qwen3-Embedding-0.6B** (639MBï¼Œé€Ÿåº¦å¿«)
- **æ‰©å±•åˆ°æ•°åƒæ¡** â†’ **Qwen3-Embedding-4B** (é«˜ç²¾åº¦å¹³è¡¡)
- **éœ€è¦é•¿æ–‡æœ¬** â†’ **BGE-M3** (æ”¯æŒ8192 tokens)

---

## 1. æ¨¡å‹è¯¦ç»†è¯„æµ‹å¯¹æ¯”

### 1.1 æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡å¯¹æ¯”è¡¨

| æ¨¡å‹ | C-MTEB/MTEBåˆ†æ•° | å‘é‡ç»´åº¦ | æœ€å¤§Tokens | æ¨¡å‹å¤§å° | ä¸­æ–‡èƒ½åŠ› | Ollamaæ”¯æŒ |
|------|----------------|---------|-----------|---------|---------|-----------|
| **Qwen3-Embedding-0.6B** | MMTEB: 64.33 | 32-1024å¯è°ƒ | 32K | 639MB (Q8) | â­â­â­â­â­ | âœ… (ç¤¾åŒºæ¨¡å‹) |
| **Qwen3-Embedding-4B** | æ›´é«˜ (æœªå…¬å¼€å…·ä½“åˆ†) | 32-1024å¯è°ƒ | 32K | ~4GB | â­â­â­â­â­ | âš ï¸ (éœ€ç¤¾åŒºå¯¼å…¥) |
| **Qwen3-Embedding-8B** | MTEB: 70.58 (å¤šè¯­è¨€ç¬¬ä¸€) | 32-1024å¯è°ƒ | 32K | ~8GB | â­â­â­â­â­ | âš ï¸ (éœ€ç¤¾åŒºå¯¼å…¥) |
| **BGE-Large-ZH-V1.5** | C-MTEB: ç¬¬ä¸€å | 1024 | 512 | 1.34GB | â­â­â­â­â­ | âš ï¸ (éœ€ç¤¾åŒºå¯¼å…¥) |
| **BGE-M3** | MIRACL/MKQA SOTA | 1024 | 8192 | 2.27GB (571MBé‡åŒ–å) | â­â­â­â­ | âœ… (å®˜æ–¹) |
| **Stella-Large-ZH-V3** | C-MTEB: 68.48 | 1792 | 512 | ~1.5GB | â­â­â­â­â­ | âš ï¸ (éœ€ç¤¾åŒºå¯¼å…¥) |
| **Jina-Embeddings-V2-Base-ZH** | MTEBä¸Ada-002ç›¸å½“ | 768 | 8192 | ~640MB | â­â­â­â­ | âœ… (ç¤¾åŒºæ¨¡å‹) |
| **Multilingual-E5-Large** | MTEBç«äº‰åŠ›å¼º | 1024 | 512 | ~2GB | â­â­â­ | âš ï¸ (éœ€ç¤¾åŒºå¯¼å…¥) |
| **Nomic-Embed-Text** | 71% (æŸRAGæµ‹è¯•) | 768 | 8192 | ~548MB | âŒ (ä¸æ”¯æŒä¸­æ–‡) | âœ… (å®˜æ–¹æ¨è) |

**å…³é”®å‘ç°**ï¼š
- Qwen3ç³»åˆ—åœ¨2024å¹´6æœˆå‘å¸ƒåè¿…é€Ÿå é¢†MTEBå¤šè¯­è¨€æ¦œé¦–
- BGE-Large-ZH-V1.5åœ¨çº¯ä¸­æ–‡åœºæ™¯ä»ä¿æŒC-MTEBç¬¬ä¸€
- Nomic-Embed-Textè™½ç„¶æ˜¯Ollamaé»˜è®¤æ¨èï¼Œä½†**ä¸æ”¯æŒä¸­æ–‡**

---

### 1.2 è¯¦ç»†æ¨¡å‹åˆ†æ

#### ğŸ† Top 1: Qwen3-Embedding (é˜¿é‡Œå·´å·´)

**æ¨¡å‹å®¶æ—**: 0.6B / 4B / 8B ä¸‰ä¸ªè§„æ ¼

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
1. **ä¸­è‹±æ–‡SOTA**: åœ¨MTEBå’ŒC-MTEBè¯„ä¼°ä¸­è¡¨ç°ä¼˜å¼‚
2. **çµæ´»è¾“å‡ºç»´åº¦**: 32-1024å¯è°ƒï¼Œæ–¹ä¾¿å­˜å‚¨ä¼˜åŒ–
3. **è¶…é•¿ä¸Šä¸‹æ–‡**: æ”¯æŒ32,000 tokensï¼ˆå¯¹è¯å†å²å‹å¥½ï¼‰
4. **æŒ‡ä»¤æ„ŸçŸ¥**: æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰æŒ‡ä»¤ä¼˜åŒ–ç‰¹å®šé¢†åŸŸ
5. **å¼€æºå‹å¥½**: Apache 2.0è®¸å¯ï¼Œå•†ä¸šä½¿ç”¨æ— éšœç¢

**åŸºå‡†æµ‹è¯•**ï¼š
- Qwen3-Embedding-8B: MTEBå¤šè¯­è¨€æ¦œç¬¬ä¸€ (70.58åˆ†)
- Qwen3-Embedding-0.6B: MMTEB 64.33åˆ†ï¼ˆæ¯”BGE-M3æå‡7.9%ï¼‰
- Qwen3-Reranker-8B: C-MTEB-R 77.45åˆ†

**å®é™…æ€§èƒ½**ï¼ˆç”Ÿäº§ç¯å¢ƒæŠ¥å‘Šï¼‰ï¼š
- ä½¿ç”¨query/documentæç¤ºè¯ä¼˜åŒ–åï¼ŒFAQæ£€ç´¢ç›¸å…³æ€§æå‡22%
- CPUæ¨ç†å»¶è¿Ÿ: 380ms/æŸ¥è¯¢
- GPU (T4)æ¨ç†å»¶è¿Ÿ: 85ms/æŸ¥è¯¢
- é…åˆRerankeråç­”æ¡ˆè´¨é‡æå‡31%

**éƒ¨ç½²æ–¹å¼**ï¼š
```bash
# æ–¹å¼1: HuggingFace Transformers
pip install transformers
# ä½¿ç”¨æ¨¡å‹: Qwen/Qwen3-Embedding-0.6B / 4B / 8B

# æ–¹å¼2: Ollamaï¼ˆéœ€ç¤¾åŒºæ¨¡å‹æ–‡ä»¶ï¼‰
# å½“å‰å®˜æ–¹æœªæ”¶å½•ï¼Œéœ€æ‰‹åŠ¨åˆ›å»ºModelfileæˆ–ç­‰å¾…ç¤¾åŒºæ”¯æŒ
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… ä¸­è‹±æ··åˆå¯¹è¯å†å²æ£€ç´¢
- âœ… é•¿æ–‡æœ¬åœºæ™¯ï¼ˆå•æ¡å¯¹è¯>1000å­—ï¼‰
- âœ… éœ€è¦çµæ´»ç»´åº¦è°ƒæ•´çš„åœºæ™¯
- âœ… å¯¹æ£€ç´¢ç²¾åº¦è¦æ±‚æé«˜çš„åœºæ™¯

**å·²çŸ¥é—®é¢˜**ï¼ˆGitHub Issuesï¼‰ï¼š
- Ollamaé›†æˆå­˜åœ¨"Error 400 Object Type Not Iterable"é—®é¢˜ï¼ˆ2024å¹´12æœˆï¼‰
- ä¸åŒç»´åº¦æ¨¡å‹é—´å‘é‡ä¸å…¼å®¹ï¼ˆ0.6Bçš„åµŒå…¥ä¸èƒ½ç”¨äº4Bçš„æ£€ç´¢ï¼‰

---

#### ğŸ¥ˆ Top 2: BGE-Large-ZH-V1.5 (åŒ—äº¬æ™ºæºBAAI)

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
1. **ä¸­æ–‡ä¸“ç”¨ä¼˜åŒ–**: C-MTEB 31ä¸ªæ•°æ®é›†æ€»æ’åç¬¬ä¸€
2. **æ”¹è¿›çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ**: V1.5ä¿®å¤äº†V1çš„åˆ†å¸ƒé—®é¢˜
3. **æ— éœ€æŒ‡ä»¤å‰ç¼€**: ç®€åŒ–äº†ä½¿ç”¨æµç¨‹
4. **æˆç†Ÿç”Ÿæ€**: FlagEmbeddingæ¡†æ¶å®Œæ•´æ”¯æŒ

**åŸºå‡†æµ‹è¯•**ï¼š
- C-MTEB: å…­å¤§ä»»åŠ¡ï¼ˆåˆ†ç±»/èšç±»/æ£€ç´¢/STS/é…å¯¹åˆ†ç±»/é‡æ’ï¼‰å…¨é¢é¢†å…ˆ
- ç›¸æ¯”å‰ä»£æå‡è¶…è¿‡10%

**æ¨¡å‹è§„æ ¼**ï¼š
- å‚æ•°: ~326M
- ç»´åº¦: 1024
- æœ€å¤§tokens: 512
- ä¸‹è½½å¤§å°: 1.34GB

**éƒ¨ç½²æ–¹å¼**ï¼š
```bash
# æ–¹å¼1: FlagEmbeddingï¼ˆå®˜æ–¹æ¨èï¼‰
pip install -U FlagEmbedding
# from FlagEmbedding import BGEM3FlagModel

# æ–¹å¼2: Sentence-Transformers
pip install sentence-transformers
# model = SentenceTransformer('BAAI/bge-large-zh-v1.5')

# æ–¹å¼3: Ollamaï¼ˆéœ€æ‰‹åŠ¨å¯¼å…¥ï¼‰
# å½“å‰å®˜æ–¹æœªæ”¶å½•ï¼Œéœ€ç¤¾åŒºModelfile
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… çº¯ä¸­æ–‡æˆ–ä¸­æ–‡ä¸ºä¸»çš„å¯¹è¯
- âœ… å•æ¡å¯¹è¯<512 tokensï¼ˆçº¦350-400æ±‰å­—ï¼‰
- âœ… è¿½æ±‚æœ€é«˜ä¸­æ–‡æ£€ç´¢ç²¾åº¦
- âœ… éœ€è¦æˆç†Ÿå·¥å…·é“¾æ”¯æŒ

**é™åˆ¶**ï¼š
- âš ï¸ 512 tokené™åˆ¶å¯èƒ½ä¸é€‚åˆé•¿å¯¹è¯
- âš ï¸ ä¸æ”¯æŒå¤šè¯­è¨€æ··åˆï¼ˆä»…ä¸­è‹±ï¼‰

---

#### ğŸ¥‰ Top 3: BGE-M3 (åŒ—äº¬æ™ºæºBAAI)

**å…¨å**: Multi-linguality, Multi-granularities, Multi-Functionality

**æ ¸å¿ƒä¼˜åŠ¿**ï¼š
1. **æ··åˆæ£€ç´¢**: åŒæ—¶æ”¯æŒå¯†é›†/ç¨€ç–/å¤šå‘é‡æ£€ç´¢
2. **100+è¯­è¨€**: çœŸæ­£çš„å¤šè¯­è¨€æ”¯æŒ
3. **8192 tokens**: é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›å¼º
4. **é«˜æ•ˆé‡åŒ–**: ONNXé‡åŒ–åå‡å°‘75%ä½“ç§¯ï¼Œ3å€é€Ÿåº¦æå‡

**åŸºå‡†æµ‹è¯•**ï¼š
- MIRACLï¼ˆå¤šè¯­è¨€æ£€ç´¢ï¼‰: SOTA
- MKQAï¼ˆè·¨è¯­è¨€QAï¼‰: SOTA
- æŸRAGè¯„æµ‹: æ£€ç´¢å‡†ç¡®ç‡72%ï¼ˆæµ‹è¯•é›†æœ€é«˜ï¼‰
- é•¿é—®é¢˜æ£€ç´¢å‡†ç¡®ç‡: 92.5%

**æ¨¡å‹è§„æ ¼**ï¼š
- å‚æ•°: 0.6B
- ç»´åº¦: 1024
- æœ€å¤§tokens: 8192
- ä¸‹è½½å¤§å°: 2.27GBï¼ˆé‡åŒ–å571MBï¼‰

**æ€§èƒ½ä¼˜åŒ–**ï¼š
```python
# ONNXé‡åŒ–åæ€§èƒ½
- æ¨¡å‹å¤§å°: 2272MB â†’ 571MB (å‡å°‘75%)
- æ¨ç†é€Ÿåº¦: æå‡3å€
- ç²¾åº¦æŸå¤±: ç¨€ç–0.15%, å¯†é›†0.65%ï¼ˆå¯å¿½ç•¥ï¼‰
```

**éƒ¨ç½²æ–¹å¼**ï¼š
```bash
# Ollamaï¼ˆå®˜æ–¹æ”¯æŒï¼‰
ollama pull bge-m3

# ä½¿ç”¨ç¤ºä¾‹
curl http://localhost:11434/api/embeddings -d '{
  "model": "bge-m3",
  "prompt": "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"
}'
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… éœ€è¦æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰
- âœ… é•¿å¯¹è¯æ–‡æœ¬ï¼ˆ>1000 tokensï¼‰
- âœ… å¤šè¯­è¨€æ··åˆå¯¹è¯
- âœ… èµ„æºå—é™ç¯å¢ƒï¼ˆé‡åŒ–åï¼‰

**æƒè¡¡**ï¼š
- âš ï¸ ä¸­æ–‡çº¯æ£€ç´¢æ€§èƒ½ç•¥ä½äºBGE-Large-ZH-V1.5
- âš ï¸ é…ç½®å¤æ‚åº¦é«˜äºå•ä¸€æ£€ç´¢æ¨¡å‹

---

#### å…¶ä»–å€™é€‰æ¨¡å‹ç®€è¯„

**Stella-Large-ZH-V3** (Infgrad)
- C-MTEB: 68.48åˆ†
- 1792ç»´ï¼ˆé«˜ç»´åº¦ç‰¹æ®Šåœºæ™¯ï¼‰
- æ”¯æŒå¯¹è¯ç¼–ç ä¼˜åŒ–
- éƒ¨ç½²éš¾åº¦: ä¸­ç­‰ï¼ˆéœ€HuggingFaceï¼‰
- é€‚åˆ: å¯¹ç»´åº¦æœ‰ç‰¹æ®Šè¦æ±‚çš„åœºæ™¯

**Jina-Embeddings-V2-Base-ZH** (Jina AI)
- ä¸­è‹±åŒè¯­ï¼Œ8192 tokens
- 161Må‚æ•°ï¼ˆè½»é‡çº§ï¼‰
- Ollamaç¤¾åŒºæ”¯æŒ: `ollama pull EntropyYue/jina-embeddings-v2-base-zh`
- é€‚åˆ: å¿«é€Ÿéƒ¨ç½²ã€è·¨è¯­è¨€æ£€ç´¢

**Multilingual-E5-Large** (å¾®è½¯)
- 100è¯­è¨€æ”¯æŒ
- 1024ç»´ï¼Œ560Må‚æ•°
- ä¸­æ–‡æ€§èƒ½: ä¸­ç­‰ï¼ˆä½äºä¸“ç”¨ä¸­æ–‡æ¨¡å‹ï¼‰
- é€‚åˆ: å¼ºå¤šè¯­è¨€éœ€æ±‚

---

## 2. æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆè¯¦è§£

### 2.1 Ollamaéƒ¨ç½²ï¼ˆæ¨èæ–¹æ¡ˆï¼‰

**å®˜æ–¹æ”¯æŒçš„åµŒå…¥æ¨¡å‹**ï¼š
```bash
# æŸ¥çœ‹æ‰€æœ‰åµŒå…¥æ¨¡å‹
ollama list | grep embed

# æ¨èå®‰è£…ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
ollama pull bge-m3                    # å¤šåŠŸèƒ½æ··åˆæ£€ç´¢
ollama pull dmeta-embedding-zh        # è½»é‡ä¸­æ–‡ï¼ˆ100Mï¼‰
ollama pull jina-embeddings-v2-base-zh # é•¿æ–‡æœ¬ä¸­è‹±åŒè¯­
ollama pull granite-embedding:278m    # IBMå¤šè¯­è¨€ï¼ˆå«ä¸­æ–‡ï¼‰
```

**æ³¨æ„äº‹é¡¹**ï¼š
- âš ï¸ **nomic-embed-textä¸æ”¯æŒä¸­æ–‡**ï¼ˆè™½ç„¶æ˜¯Ollamaæ¨èï¼‰
- âš ï¸ BGE-Large-ZH-V1.5å’ŒQwen3ç³»åˆ—éœ€æ‰‹åŠ¨åˆ›å»ºModelfile

**æ‰‹åŠ¨å¯¼å…¥æ¨¡å‹åˆ°Ollama**ï¼ˆé«˜çº§ï¼‰ï¼š
```bash
# æ­¥éª¤1: ä¸‹è½½HuggingFaceæ¨¡å‹
git clone https://huggingface.co/BAAI/bge-large-zh-v1.5

# æ­¥éª¤2: åˆ›å»ºModelfile
cat > Modelfile <<EOF
FROM ./bge-large-zh-v1.5
TEMPLATE """{{ .Prompt }}"""
PARAMETER temperature 0
EOF

# æ­¥éª¤3: æ„å»ºOllamaæ¨¡å‹
ollama create bge-large-zh-v1.5 -f Modelfile
```

---

### 2.2 HuggingFace Transformerséƒ¨ç½²

**æ¨èç”¨äº**: Qwen3ã€BGEç³»åˆ—ã€Stellaç­‰

```python
# å®‰è£…ä¾èµ–
pip install transformers torch sentence-transformers

# ä½¿ç”¨Qwen3-Embedding-0.6B
from transformers import AutoModel, AutoTokenizer

model_name = "Qwen/Qwen3-Embedding-0.6B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# ç”ŸæˆåµŒå…¥
texts = ["ä½ å¥½ï¼Œè¿™æ˜¯ç¬¬ä¸€å¥è¯", "è¿™æ˜¯ç¬¬äºŒå¥è¯"]
inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
outputs = model(**inputs)
embeddings = outputs.last_hidden_state.mean(dim=1)  # æ± åŒ–
```

**ä¼˜åŒ–æŠ€å·§**ï¼š
```python
# 1. ä½¿ç”¨FP16å‡å°‘å†…å­˜
model.half()  # å‡å°‘50%å†…å­˜å ç”¨

# 2. æ‰¹å¤„ç†åŠ é€Ÿ
batch_size = 32
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    # å¤„ç†æ‰¹æ¬¡

# 3. GPUåŠ é€Ÿ
model.to('cuda')
```

---

### 2.3 FastEmbedéƒ¨ç½²ï¼ˆæœ€å¿«æ–¹æ¡ˆï¼‰

**ç‰¹ç‚¹**: ä¸“ä¸ºé€Ÿåº¦ä¼˜åŒ–çš„åµŒå…¥åº“

```python
# å®‰è£…
pip install fastembed

# ä½¿ç”¨BGEæ¨¡å‹
from fastembed import TextEmbedding

model = TextEmbedding(model_name="BAAI/bge-small-zh-v1.5")
embeddings = list(model.embed(["ä½ å¥½ä¸–ç•Œ", "æµ‹è¯•æ–‡æœ¬"]))
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
- æ¯”æ ‡å‡†Transformerså¿«2-3å€
- å†…å­˜å ç”¨å‡å°‘30-40%
- æ”¯æŒæµå¼å¤„ç†å¤§æ‰¹é‡æ–‡æœ¬

---

### 2.4 ONNXé‡åŒ–éƒ¨ç½²ï¼ˆç”Ÿäº§ä¼˜åŒ–ï¼‰

**é€‚ç”¨åœºæ™¯**: CPUæ¨ç†ã€å†…å­˜å—é™ã€é«˜å¹¶å‘

```python
# å®‰è£…
pip install optimum[onnxruntime]

# è½¬æ¢ä¸ºONNXå¹¶é‡åŒ–
from optimum.onnxruntime import ORTModelForFeatureExtraction

model = ORTModelForFeatureExtraction.from_pretrained(
    "BAAI/bge-large-zh-v1.5",
    export=True,
    provider="CPUExecutionProvider"
)

# INT8é‡åŒ–
from optimum.onnxruntime import ORTQuantizer
quantizer = ORTQuantizer.from_pretrained(model)
quantizer.quantize(save_dir="./quantized_model")
```

**é‡åŒ–æ•ˆæœ**ï¼ˆåŸºäºBGE-M3å®æµ‹ï¼‰ï¼š
- æ¨¡å‹å¤§å°: -75% (2.27GB â†’ 571MB)
- æ¨ç†é€Ÿåº¦: +3å€
- ç²¾åº¦æŸå¤±: <1% (0.15%-0.65%)
- å†…å­˜å ç”¨: -20%

---

## 3. æ€§èƒ½åŸºå‡†æµ‹è¯•

### 3.1 æ£€ç´¢è´¨é‡å¯¹æ¯”ï¼ˆRAGåœºæ™¯å®æµ‹ï¼‰

**æµ‹è¯•æ•°æ®é›†**: Pinecone RAGè¯„ä¼°ï¼ˆå¤šæºæ··åˆï¼‰

| æ¨¡å‹ | æ•´ä½“å‡†ç¡®ç‡ | çŸ­é—®é¢˜ | é•¿é—®é¢˜ | ä¸Šä¸‹æ–‡å¯†é›†å‹ |
|------|----------|--------|--------|------------|
| BGE-M3 | 72.0% | 68% | 92.5% | æœ€ä¼˜ |
| Nomic-Embed-Text | 57.25% | 71% | 45% | ä¸€èˆ¬ |
| Mxbai-Embed-Large | ~70% | - | 82.5% | è‰¯å¥½ |

**ä¸­æ–‡ä¸“é¡¹æµ‹è¯•**ï¼ˆéå®˜æ–¹ç¤¾åŒºæŠ¥å‘Šï¼‰ï¼š
- Qwen3-0.6B + Reranker: FAQå‡†ç¡®ç‡æå‡22%
- BGE-Large-ZH-V1.5: çº¯ä¸­æ–‡æ£€ç´¢æ¯”BGE-M3é«˜3-5%
- Jina-V2-Base-ZH: è·¨è¯­è¨€æ£€ç´¢æ¯”å•è¯­æ¨¡å‹é«˜8-12%

---

### 3.2 æ¨ç†é€Ÿåº¦å¯¹æ¯”ï¼ˆCPU vs GPUï¼‰

**æµ‹è¯•ç¡¬ä»¶**: Intel CPU (VNNIæ”¯æŒ) / NVIDIA T4 GPU

| æ¨¡å‹ | CPU (ms/query) | GPU (ms/query) | æ‰¹å¤„ç†åŠ é€Ÿ |
|------|---------------|---------------|-----------|
| Qwen3-0.6B | 380 | 85 | 4.5x |
| BGE-Large-ZH-V1.5 | ~450 | ~120 | 3.8x |
| BGE-M3 | 520 | 140 | 3.7x |
| Jina-V2-Base-ZH | 280 | 75 | 3.7x |

**é‡åŒ–åŠ é€Ÿ**ï¼ˆåŸºäºINT8ï¼‰ï¼š
- CPU VNNI: 1.6x - 4.5xåŠ é€Ÿ
- GPU: 1.2x - 2.3xåŠ é€Ÿ
- é™æ€åµŒå…¥: CPU 100-400x, GPU 10-25xï¼ˆç‰¹æ®Šåœºæ™¯ï¼‰

---

### 3.3 æ¨¡å‹å¤§å°ä¸å†…å­˜å ç”¨

**ä¸‹è½½å¤§å°å¯¹æ¯”**ï¼š
```
Qwen3-0.6B (Q8é‡åŒ–): 639MB  â­ æœ€è½»é‡SOTA
Jina-V2-Base-ZH:      ~640MB
BGE-Large-ZH-V1.5:    1.34GB
Stella-Large-ZH-V3:   ~1.5GB
BGE-M3:               2.27GB (é‡åŒ–å571MB)
Qwen3-4B:             ~4GB
Qwen3-8B:             ~8GB
```

**è¿è¡Œæ—¶å†…å­˜å ç”¨**ï¼ˆFP32 vs FP16ï¼‰ï¼š
- 0.6Bæ¨¡å‹: 2.4GB (FP32) / 1.2GB (FP16)
- 4Bæ¨¡å‹: 16GB (FP32) / 8GB (FP16)
- 8Bæ¨¡å‹: 32GB (FP32) / 16GB (FP16)

**å»ºè®®é…ç½®**ï¼š
- æ™®é€šå·¥ä½œç«™ï¼ˆ16GBå†…å­˜ï¼‰: Qwen3-0.6B æˆ– BGE-Large-ZH-V1.5
- é«˜é…å·¥ä½œç«™ï¼ˆ32GBå†…å­˜ï¼‰: Qwen3-4B
- æœåŠ¡å™¨ï¼ˆ64GB+å†…å­˜ï¼‰: Qwen3-8B

---

## 4. é’ˆå¯¹ä½ åœºæ™¯çš„æœ€ç»ˆæ¨è

### 4.1 åœºæ™¯åˆ†æ

**ä½ çš„éœ€æ±‚ç‰¹å¾**ï¼š
1. æ•°æ®ç±»å‹: ä¸ªäººå¯¹è¯å†å²ï¼ˆä¸­æ–‡ä¸ºä¸»ï¼Œå¯èƒ½ä¸­è‹±æ··åˆï¼‰
2. æ•°æ®è§„æ¨¡: åˆæœŸ500æ¡ â†’ æœ€ç»ˆæ•°åƒè‡³ä¸Šä¸‡æ¡
3. æ£€ç´¢ç›®æ ‡: è¯­ä¹‰ç›¸ä¼¼åº¦æ£€ç´¢ï¼ˆæ‰¾åˆ°ç›¸å…³å†å²å¯¹è¯ï¼‰
4. éƒ¨ç½²è¦æ±‚: æœ¬åœ°éƒ¨ç½²ï¼ˆéšç§ä¼˜å…ˆï¼‰
5. ç¡¬ä»¶ç¯å¢ƒ: æ™®é€šå·¥ä½œç«™Windows + GPUï¼ˆå¯é€‰ï¼‰

---

### 4.2 åˆ†é˜¶æ®µæ¨èæ–¹æ¡ˆ

#### ğŸ“ **æ–¹æ¡ˆA: åˆæœŸå¿«é€Ÿå¯åŠ¨ï¼ˆæ¨èï¼‰**

**æ¨¡å‹é€‰æ‹©**: **Qwen3-Embedding-0.6B** + **Ollama/HuggingFace**

**ç†ç”±**ï¼š
1. âœ… ä½“ç§¯å°ï¼ˆ639MBï¼‰ï¼Œæ™®é€šå·¥ä½œç«™è½»æ¾è¿è¡Œ
2. âœ… ä¸­æ–‡SOTAæ€§èƒ½ï¼ˆMMTEB 64.33ï¼‰
3. âœ… 500æ¡æ•°æ®æ£€ç´¢é€Ÿåº¦æå¿«ï¼ˆCPU <380msï¼‰
4. âœ… å¯è°ƒç»´åº¦ï¼ˆå»ºè®®åˆæœŸç”¨512ç»´èŠ‚çœå­˜å‚¨ï¼‰
5. âœ… Apache 2.0å¼€æºè®¸å¯

**éƒ¨ç½²æ­¥éª¤**ï¼š
```bash
# æ–¹å¼1: HuggingFaceï¼ˆæ¨èï¼‰
pip install transformers torch

# Pythonä½¿ç”¨ç¤ºä¾‹
from transformers import AutoModel, AutoTokenizer
model = AutoModel.from_pretrained("Qwen/Qwen3-Embedding-0.6B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-Embedding-0.6B")

# æ–¹å¼2: ç­‰å¾…Ollamaç¤¾åŒºæ”¯æŒï¼ˆç®€åŒ–éƒ¨ç½²ï¼‰
# å…³æ³¨ https://github.com/ollama/ollama/issues
```

**é¢„æœŸæ€§èƒ½**ï¼ˆ500-1000æ¡ï¼‰ï¼š
- æ„å»ºç´¢å¼•: <5åˆ†é’Ÿ
- å•æ¬¡æŸ¥è¯¢: <0.5ç§’
- å†…å­˜å ç”¨: 1.5GBå·¦å³

---

#### ğŸ“ **æ–¹æ¡ˆB: è¿½æ±‚æè‡´ä¸­æ–‡è´¨é‡**

**æ¨¡å‹é€‰æ‹©**: **BGE-Large-ZH-V1.5** + **FlagEmbeddingæ¡†æ¶**

**ç†ç”±**ï¼š
1. âœ… C-MTEBçº¯ä¸­æ–‡ç¬¬ä¸€
2. âœ… æˆç†Ÿå·¥å…·é“¾ï¼ˆFlagEmbeddingï¼‰
3. âœ… æ— éœ€æŒ‡ä»¤å‰ç¼€ï¼Œä½¿ç”¨ç®€å•
4. âœ… 1.34GBé€‚ä¸­ï¼Œå•å¡å¯è·‘

**éƒ¨ç½²æ­¥éª¤**ï¼š
```bash
pip install -U FlagEmbedding

# Pythonä½¿ç”¨
from FlagEmbedding import FlagModel
model = FlagModel('BAAI/bge-large-zh-v1.5', use_fp16=True)

# ç”ŸæˆåµŒå…¥
embeddings = model.encode(["å¯¹è¯1", "å¯¹è¯2"])
```

**é€‚ç”¨æ¡ä»¶**ï¼š
- âš ï¸ å•æ¡å¯¹è¯<400æ±‰å­—ï¼ˆ512 tokené™åˆ¶ï¼‰
- âš ï¸ çº¯ä¸­æ–‡æˆ–ä¸­æ–‡å æ¯”>90%

---

#### ğŸ“ **æ–¹æ¡ˆC: é•¿æ–‡æœ¬ + å¤šè¯­è¨€åœºæ™¯**

**æ¨¡å‹é€‰æ‹©**: **BGE-M3** + **Ollama**

**ç†ç”±**ï¼š
1. âœ… æ”¯æŒ8192 tokensï¼ˆé€‚åˆé•¿å¯¹è¯ï¼‰
2. âœ… Ollamaå®˜æ–¹æ”¯æŒï¼ˆä¸€é”®éƒ¨ç½²ï¼‰
3. âœ… æ··åˆæ£€ç´¢ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰
4. âœ… é‡åŒ–åä»…571MB

**éƒ¨ç½²æ­¥éª¤**ï¼š
```bash
# Ollamaä¸€é”®å®‰è£…
ollama pull bge-m3

# Pythonè°ƒç”¨
import ollama
response = ollama.embeddings(model='bge-m3', prompt='ä½ çš„å¯¹è¯æ–‡æœ¬')
```

**é€‚ç”¨æ¡ä»¶**ï¼š
- âœ… å•æ¡å¯¹è¯>500æ±‰å­—
- âœ… ä¸­è‹±æ··åˆæˆ–å¤šè¯­è¨€
- âœ… éœ€è¦å…³é”®è¯è¾…åŠ©æ£€ç´¢

---

### 4.3 æ‰©å±•ç­–ç•¥ï¼ˆæ•°åƒæ¡â†’ä¸Šä¸‡æ¡ï¼‰

**é˜¶æ®µ1: 500-2000æ¡** â†’ ä¿æŒQwen3-0.6B
- å•æœºå®Œå…¨å¤Ÿç”¨
- è€ƒè™‘å‘é‡æ•°æ®åº“ï¼ˆChroma/Milvusï¼‰

**é˜¶æ®µ2: 2000-5000æ¡** â†’ å‡çº§Qwen3-4Bæˆ–æ·»åŠ Reranker
- æ£€ç´¢ç²¾åº¦æå‡
- å¯é€‰: Qwen3-Reranker-4Bï¼ˆäºŒæ¬¡ç²¾æ’ï¼‰

**é˜¶æ®µ3: 5000+æ¡** â†’ ä¼˜åŒ–æ¶æ„
- å‘é‡æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–ï¼ˆHNSW/IVFï¼‰
- æ··åˆæ£€ç´¢ï¼ˆåµŒå…¥+BM25ï¼‰
- åˆ†ç‰‡ç­–ç•¥ï¼ˆæŒ‰æ—¶é—´/ä¸»é¢˜åˆ†åº“ï¼‰

---

### 4.4 ç»„åˆæ¨èé…ç½®

#### ğŸ”¥ **æœ€ä½³ç»¼åˆæ–¹æ¡ˆ**

```yaml
åµŒå…¥æ¨¡å‹: Qwen3-Embedding-0.6B
å‘é‡ç»´åº¦: 512 (åˆæœŸ) â†’ 768 (æ‰©å±•å)
å‘é‡æ•°æ®åº“: ChromaDB (æœ¬åœ°è½»é‡) / Milvus (å¤§è§„æ¨¡)
é‡æ’åºå™¨: Qwen3-Reranker-0.6B (å¯é€‰)
åˆ†å—ç­–ç•¥:
  - å•è½®å¯¹è¯: ä¿æŒå®Œæ•´
  - å¤šè½®å¯¹è¯: æŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†å—ï¼ˆ3-5è½®ä¸ºä¸€ç»„ï¼‰
  - é‡å : 20% (ä¿ç•™ä¸Šä¸‹æ–‡)
æ£€ç´¢æµç¨‹:
  1. å‘é‡æ£€ç´¢ Top 20
  2. Rerankerç²¾æ’ Top 5
  3. è¿”å›æœ€ç›¸å…³ç»“æœ
```

**æˆæœ¬ä¼°ç®—**ï¼š
- ç¡¬ä»¶: 16GBå†…å­˜å·¥ä½œç«™å³å¯
- è½¯ä»¶: å…¨å…è´¹å¼€æº
- æ€§èƒ½: å•æ¬¡æŸ¥è¯¢<1ç§’ï¼ˆå«Rerankerï¼‰

---

## 5. åˆ†å—ï¼ˆChunkingï¼‰æœ€ä½³å®è·µ

### 5.1 å¯¹è¯å†å²ä¸“ç”¨ç­–ç•¥

**æ¨èæ–¹æ¡ˆ: è¯­ä¹‰å¯¹è¯å•å…ƒåˆ†å—**

```python
# ç¤ºä¾‹: å¤šè½®å¯¹è¯åˆ†å—
conversation = [
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"},
    {"role": "user", "content": "æˆ‘æƒ³äº†è§£Python"},
    {"role": "assistant", "content": "Pythonæ˜¯ä¸€é—¨...ï¼ˆé•¿å›å¤ï¼‰"},
]

# ç­–ç•¥: æ¯3-5ä¸ªå¯¹è¯è½®æ¬¡ä¸ºä¸€ä¸ªchunk
chunks = []
for i in range(0, len(conversation), 6):  # 6æ¡=3è½®
    chunk = conversation[i:i+6]
    chunks.append({
        "text": format_conversation(chunk),
        "metadata": {"start_turn": i//2, "end_turn": (i+6)//2}
    })
```

**å‚æ•°å»ºè®®**ï¼š
- **Chunkå¤§å°**: 3-5è½®å¯¹è¯ï¼ˆçº¦200-500 tokensï¼‰
- **é‡å **: 1è½®ï¼ˆ20%é‡å ï¼Œä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
- **æœ€å¤§é•¿åº¦**:
  - BGE-Large-ZH-V1.5: 400æ±‰å­—
  - BGE-M3/Qwen3: 1000-2000æ±‰å­—
  - Jina-V2: 3000-4000æ±‰å­—

---

### 5.2 å•æ¡é•¿å¯¹è¯å¤„ç†

**åœºæ™¯**: å•æ¡å¯¹è¯>1000å­—

**ç­–ç•¥**: é€’å½’è¯­ä¹‰åˆ†å— + 20%é‡å 

```python
# ä¼ªä»£ç 
def chunk_long_conversation(text, max_tokens=512, overlap=0.2):
    """
    æŒ‰å¥å­è¾¹ç•Œåˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
    """
    sentences = split_by_sentence(text)
    chunks = []
    current_chunk = []
    current_tokens = 0

    for sentence in sentences:
        tokens = count_tokens(sentence)
        if current_tokens + tokens > max_tokens:
            # ä¿å­˜å½“å‰chunk
            chunks.append(join(current_chunk))
            # ä¿ç•™é‡å éƒ¨åˆ†
            overlap_size = int(len(current_chunk) * overlap)
            current_chunk = current_chunk[-overlap_size:]
            current_tokens = count_tokens(current_chunk)

        current_chunk.append(sentence)
        current_tokens += tokens

    return chunks
```

---

## 6. å®æˆ˜éƒ¨ç½²ç¤ºä¾‹

### 6.1 å®Œæ•´RAGæµç¨‹ï¼ˆQwen3 + ChromaDBï¼‰

```python
# å®‰è£…ä¾èµ–
# pip install chromadb transformers torch

import chromadb
from transformers import AutoModel, AutoTokenizer
import torch

# 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
model_name = "Qwen/Qwen3-Embedding-0.6B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model.eval()

def get_embeddings(texts):
    """ç”ŸæˆåµŒå…¥å‘é‡"""
    inputs = tokenizer(texts, return_tensors='pt',
                      padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    # Mean pooling
    embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings.numpy()

# 2. åˆå§‹åŒ–ChromaDB
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(
    name="conversation_history",
    metadata={"hnsw:space": "cosine"}
)

# 3. æ·»åŠ å¯¹è¯å†å²
conversations = [
    "ç”¨æˆ·: ä½ å¥½\nåŠ©æ‰‹: ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ",
    "ç”¨æˆ·: ä»€ä¹ˆæ˜¯RAG?\nåŠ©æ‰‹: RAGæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆ...",
    # ... æ›´å¤šå¯¹è¯
]

embeddings = get_embeddings(conversations)
collection.add(
    embeddings=embeddings.tolist(),
    documents=conversations,
    ids=[f"conv_{i}" for i in range(len(conversations))]
)

# 4. æ£€ç´¢ç›¸ä¼¼å¯¹è¯
query = "å¦‚ä½•ä½¿ç”¨RAGç³»ç»Ÿ"
query_embedding = get_embeddings([query])[0]

results = collection.query(
    query_embeddings=[query_embedding.tolist()],
    n_results=5
)

print("æœ€ç›¸å…³çš„å¯¹è¯:")
for doc, distance in zip(results['documents'][0], results['distances'][0]):
    print(f"ç›¸ä¼¼åº¦: {1-distance:.3f}")
    print(f"å¯¹è¯: {doc}\n")
```

---

### 6.2 æ€§èƒ½ä¼˜åŒ–ç‰ˆï¼ˆæ‰¹å¤„ç† + GPUï¼‰

```python
import torch
from torch.utils.data import DataLoader, Dataset

class ConversationDataset(Dataset):
    def __init__(self, texts):
        self.texts = texts

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.texts[idx]

# GPUåŠ é€Ÿ
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# æ‰¹å¤„ç†åµŒå…¥ç”Ÿæˆ
def batch_encode(texts, batch_size=32):
    dataset = ConversationDataset(texts)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    all_embeddings = []
    for batch in dataloader:
        inputs = tokenizer(batch, return_tensors='pt',
                          padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1)
        all_embeddings.append(embeddings.cpu())

    return torch.cat(all_embeddings, dim=0).numpy()

# å¤„ç†10000æ¡å¯¹è¯
large_dataset = ["å¯¹è¯" + str(i) for i in range(10000)]
embeddings = batch_encode(large_dataset, batch_size=64)
# é¢„æœŸæ—¶é—´: GPU <2åˆ†é’Ÿ, CPU <15åˆ†é’Ÿ
```

---

## 7. å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### 7.1 Ollamaç›¸å…³

**Q: Qwen3æ¨¡å‹å¦‚ä½•å¯¼å…¥Ollama?**

A: å½“å‰Ollamaå®˜æ–¹æœªæ”¶å½•ï¼Œå¯å…³æ³¨ä»¥ä¸‹æ–¹æ¡ˆï¼š
1. ç­‰å¾…ç¤¾åŒºè´¡çŒ®Modelfileï¼ˆGitHub Issues #2965ï¼‰
2. ä½¿ç”¨HuggingFaceç›´æ¥åŠ è½½ï¼ˆæ¨èï¼‰
3. ä½¿ç”¨å·²æ”¯æŒçš„dmeta-embedding-zhæˆ–jina-v2-zhï¼ˆè½»é‡æ›¿ä»£ï¼‰

**Q: Nomic-Embed-Textä¸ºä½•ä¸æ”¯æŒä¸­æ–‡ï¼Ÿ**

A: è¯¥æ¨¡å‹ä»…åœ¨è‹±æ–‡è¯­æ–™ä¸Šè®­ç»ƒï¼Œè¯è¡¨ä¸åŒ…å«ä¸­æ–‡tokenï¼Œå¼ºè¡Œä½¿ç”¨ä¼šå¯¼è‡´æœªçŸ¥tokené™çº§ï¼Œæ£€ç´¢è´¨é‡æå·®ã€‚

---

### 7.2 æ€§èƒ½ä¼˜åŒ–

**Q: å¦‚ä½•åœ¨CPUä¸ŠåŠ é€Ÿæ¨ç†ï¼Ÿ**

A: ä¸‰æ­¥ä¼˜åŒ–ç­–ç•¥ï¼š
1. ONNXé‡åŒ–ï¼ˆINT8ï¼‰: 3-4å€åŠ é€Ÿ
2. å¯ç”¨VNNIæŒ‡ä»¤é›†ï¼ˆIntel CPUï¼‰
3. æ‰¹å¤„ç† + å¤šçº¿ç¨‹

```python
# ç¤ºä¾‹: ONNXä¼˜åŒ–
from optimum.onnxruntime import ORTModelForFeatureExtraction
model = ORTModelForFeatureExtraction.from_pretrained(
    "BAAI/bge-large-zh-v1.5",
    export=True,
    provider="CPUExecutionProvider"
)
```

---

### 7.3 å†…å­˜å ç”¨

**Q: 16GBå†…å­˜å¯ä»¥è·‘å“ªäº›æ¨¡å‹ï¼Ÿ**

A: æ¨èé…ç½®ï¼š
- FP16æ¨¡å¼: Qwen3-0.6B/4B, BGE-Large-ZH-V1.5, BGE-M3, Jina-V2
- FP32æ¨¡å¼: Qwen3-0.6B, Jina-V2
- é¿å…: Qwen3-8B (éœ€32GB+)

---

## 8. æ€»ç»“ä¸è¡ŒåŠ¨å»ºè®®

### 8.1 å…³é”®è¦ç‚¹

1. **2024å¹´æœ€å¤§å˜åŒ–**: Qwen3ç³»åˆ—æ¨ªæ‰«MTEBæ¦œå•ï¼Œæˆä¸ºæ–°SOTA
2. **ä¸­æ–‡æœ€ä¼˜**: BGE-Large-ZH-V1.5ä»å æ®C-MTEBç¬¬ä¸€
3. **éƒ¨ç½²å‹å¥½**: Ollamaç”Ÿæ€æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œä½†éœ€å…³æ³¨ä¸­æ–‡æ”¯æŒ
4. **é‡åŒ–ä»·å€¼**: ONNX INT8é‡åŒ–æ˜¯CPUéƒ¨ç½²çš„æœ€ä½³å®è·µ

---

### 8.2 é’ˆå¯¹ä½ çš„ç«‹å³è¡ŒåŠ¨

**æ­¥éª¤1: å¿«é€ŸéªŒè¯ï¼ˆ1å°æ—¶å†…å®Œæˆï¼‰**
```bash
# å®‰è£…ç¯å¢ƒ
pip install transformers torch chromadb

# ä¸‹è½½Qwen3-0.6Bï¼ˆè‡ªåŠ¨ç¼“å­˜åˆ°~/.cache/huggingfaceï¼‰
# ç¬¬ä¸€æ¬¡éœ€ä¸‹è½½639MBï¼Œåç»­ç§’å¼€

# è¿è¡Œä¸Šé¢ç¬¬6.1èŠ‚çš„ç¤ºä¾‹ä»£ç 
# ç”¨ä½ çš„100-200æ¡çœŸå®å¯¹è¯æµ‹è¯•æ£€ç´¢æ•ˆæœ
```

**æ­¥éª¤2: å¯¹æ¯”æµ‹è¯•ï¼ˆå‘¨æœ«é¡¹ç›®ï¼‰**
```python
# å¯¹æ¯”ä¸‰ä¸ªæ¨¡å‹
models = [
    "Qwen/Qwen3-Embedding-0.6B",
    "BAAI/bge-large-zh-v1.5",
    "BAAI/bge-m3"
]

# å‡†å¤‡æµ‹è¯•é›†ï¼ˆ20ä¸ªæŸ¥è¯¢ + 500æ¡å¯¹è¯åº“ï¼‰
# è®°å½•: æ£€ç´¢å‡†ç¡®ç‡ã€é€Ÿåº¦ã€å†…å­˜å ç”¨
# é€‰å‡ºæœ€é€‚åˆä½ æ•°æ®ç‰¹å¾çš„æ¨¡å‹
```

**æ­¥éª¤3: ç”Ÿäº§éƒ¨ç½²ï¼ˆ1-2å‘¨ï¼‰**
- é€‰å®šæ¨¡å‹åé…ç½®å‘é‡æ•°æ®åº“
- è®¾è®¡åˆ†å—ç­–ç•¥ï¼ˆæ ¹æ®å¯¹è¯é•¿åº¦ï¼‰
- å¯é€‰: æ·»åŠ RerankeräºŒæ¬¡ç²¾æ’
- ç›‘æ§æŒ‡æ ‡: æŸ¥è¯¢å»¶è¿Ÿã€å¬å›ç‡ã€å†…å­˜

---

### 8.3 é•¿æœŸè·Ÿè¸ªå»ºè®®

**å…³æ³¨èµ„æº**ï¼š
1. **MTEBæ’è¡Œæ¦œ**: https://huggingface.co/spaces/mteb/leaderboard
2. **Ollama GitHub Issues**: å…³æ³¨ä¸­æ–‡åµŒå…¥æ¨¡å‹é›†æˆè¿›å±•
3. **Qwenå›¢é˜Ÿåšå®¢**: æ–°ç‰ˆæœ¬Qwen4å¯èƒ½å¸¦æ¥æ›´å¼ºèƒ½åŠ›
4. **FlagEmbeddingé¡¹ç›®**: BGEç³»åˆ—æ›´æ–°

**æ¯å­£åº¦é‡æ–°è¯„ä¼°**ï¼š
- æ–°æ¨¡å‹æ˜¯å¦æ˜¾è‘—è¶…è¶Šå½“å‰é€‰æ‹©ï¼ˆ+5%ä»¥ä¸Šï¼‰
- éƒ¨ç½²æˆæœ¬æ˜¯å¦é™ä½ï¼ˆé‡åŒ–æ–°æŠ€æœ¯ï¼‰
- Ollamaç”Ÿæ€æ˜¯å¦ç®€åŒ–äº†éƒ¨ç½²æµç¨‹

---

## é™„å½•

### A. æœ¯è¯­è¡¨

- **MTEB**: Massive Text Embedding Benchmarkï¼ˆå¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†ï¼‰
- **C-MTEB**: Chinese MTEBï¼ˆä¸­æ–‡åµŒå…¥åŸºå‡†ï¼Œ31æ•°æ®é›†6ä»»åŠ¡ï¼‰
- **MMTEB**: Massive Multilingual MTEBï¼ˆ1000+è¯­è¨€ï¼‰
- **HNSW**: Hierarchical Navigable Small Worldï¼ˆå‘é‡ç´¢å¼•ç®—æ³•ï¼‰
- **Reranker**: é‡æ’åºæ¨¡å‹ï¼ˆæ£€ç´¢åäºŒæ¬¡ç²¾æ’ï¼‰
- **FP16/INT8**: æµ®ç‚¹16ä½/æ•´æ•°8ä½ï¼ˆé‡åŒ–ç²¾åº¦ï¼‰
- **ONNX**: Open Neural Network Exchangeï¼ˆé€šç”¨æ¨¡å‹æ ¼å¼ï¼‰
- **VNNI**: Vector Neural Network Instructionsï¼ˆIntel CPUæŒ‡ä»¤é›†ï¼‰

---

### B. ç›¸å…³é“¾æ¥

**æ¨¡å‹ä¸‹è½½**ï¼š
- Qwen3-Embedding: https://huggingface.co/Qwen/Qwen3-Embedding-0.6B
- BGE-Large-ZH-V1.5: https://huggingface.co/BAAI/bge-large-zh-v1.5
- BGE-M3: https://huggingface.co/BAAI/bge-m3
- Jina-V2-Base-ZH: https://huggingface.co/jinaai/jina-embeddings-v2-base-zh

**å®˜æ–¹æ–‡æ¡£**ï¼š
- OllamaåµŒå…¥æ¨¡å‹: https://ollama.com/blog/embedding-models
- FlagEmbedding: https://github.com/FlagOpen/FlagEmbedding
- ChromaDB: https://docs.trychroma.com/

**åŸºå‡†æµ‹è¯•**ï¼š
- MTEBæ’è¡Œæ¦œ: https://huggingface.co/spaces/mteb/leaderboard
- C-MTEB: https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB

---

### C. ç‰ˆæœ¬è®°å½•

- **v1.0** (2025-01-20): åˆå§‹è°ƒç ”æŠ¥å‘Š
- è°ƒç ”èŒƒå›´: 2024å¹´1æœˆ - 2025å¹´1æœˆæœ€æ–°æ¨¡å‹
- æ•°æ®æ¥æº: MTEBå®˜æ–¹ã€HuggingFaceã€Arxivè®ºæ–‡ã€GitHub Issuesã€æŠ€æœ¯åšå®¢

---

**æŠ¥å‘Šç»“æŸ**

å¦‚éœ€è¿›ä¸€æ­¥éªŒè¯æˆ–æœ‰ç‰¹å®šåœºæ™¯é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶æé—®ï¼
